{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e2db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Location_model_QLE3 import RobustQLELocationModel\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# note this one uses e(y,f) = y_t - f_t for location model put use -\\rho'(e)\n",
    "\n",
    "from Gasmodel import GAS_Model\n",
    "\n",
    "class RobustQLELocationModel:\n",
    "    \n",
    "    def __init__(self, alpha_loss: float = None, c: float = None):\n",
    "        \n",
    "        self.alpha_loss = alpha_loss\n",
    "        self.c = c\n",
    "        self.params = None\n",
    "        self.param_names = ['omega', 'gamma', 'beta']\n",
    "        if alpha_loss is None:\n",
    "            self.param_names.append('alpha_loss')\n",
    "        if c is None:\n",
    "            self.param_names.append('c')\n",
    "        self.fitted_location = None\n",
    "        self.residuals = None\n",
    "    \n",
    "    def _rho_derivative(self, e: np.ndarray, alpha: float, c: float) -> np.ndarray:\n",
    "        \n",
    "        # Handle scalar inputs by converting to numpy arrays\n",
    "        if np.isscalar(e):\n",
    "            e = np.array([e])\n",
    "            scalar_input = True\n",
    "        else:\n",
    "            scalar_input = False\n",
    "            \n",
    "        if alpha == 2:\n",
    "            # L2 loss (least squares)\n",
    "            result = e / (c**2)\n",
    "        elif alpha == 0:\n",
    "            # Cauchy/Lorentzian loss\n",
    "            result = (2 * e) / (e**2 + 2 * c**2)\n",
    "        elif alpha == float('-inf'):\n",
    "            # Welsch/Leclerc loss\n",
    "            result = (e / c**2) * np.exp(-0.5 * (e/c)**2)\n",
    "        else:\n",
    "            # General case\n",
    "            result = (e / c**2) * np.power((e**2 / c**2) / np.abs(alpha-2) + 1, alpha/2 - 1)\n",
    "        \n",
    "        # Return scalar if input was scalar\n",
    "        if scalar_input:\n",
    "            return result[0]\n",
    "        return result\n",
    "    \n",
    "    def _rho_second_derivative(self, e: np.ndarray, alpha: float, c: float) -> np.ndarray:\n",
    "        \n",
    "        # Handle scalar inputs by converting to numpy arrays\n",
    "        if np.isscalar(e):\n",
    "            e = np.array([e])\n",
    "            scalar_input = True\n",
    "        else:\n",
    "            scalar_input = False\n",
    "            \n",
    "        if alpha == 2:\n",
    "            # L2 loss (least squares)\n",
    "            result = -np.ones_like(e) / (c**2)\n",
    "        elif alpha == 0:\n",
    "            # Cauchy/Lorentzian loss\n",
    "            denom = (e**2 + 2 * c**2)**2\n",
    "            result = -(2*(e**2 + 2* c**2) - 4 * e **2) / denom\n",
    "        elif alpha == float('-inf'):\n",
    "            # Welsch/Leclerc loss\n",
    "            first_term = ((e)**2 * np.exp( -((e**2)/(2* (c**2)))))/(c**4)\n",
    "            second_term = np.exp( -((e**2)/(2* (c**2))))/(c**2)\n",
    "            result = (first_term - second_term)\n",
    "        else:\n",
    "            # General case\n",
    "            e2 = (e)**2\n",
    "            denom = c**2 * np.abs(alpha - 2)\n",
    "            A = e2 / denom + 1\n",
    "\n",
    "            term1 = -2 * (alpha/2 - 1) * A**(alpha/2 - 2) * e2\n",
    "            term1 /= (c**4 * np.abs(alpha - 2))\n",
    "\n",
    "            term2 = - A**(alpha/2 - 1) / (c**2)\n",
    "\n",
    "            return term1 + term2\n",
    "            \n",
    "        \n",
    "        # Return scalar if input was scalar\n",
    "        if scalar_input:\n",
    "            return result[0]\n",
    "        return result\n",
    "    \n",
    "    def dpsi_dalpha(self, e_t, c, alpha_loss):\n",
    "        \n",
    "        # define a small neighborhood around the problematic points:\n",
    "        eps = 1e-4\n",
    "\n",
    "        # if α is within ±eps of 2, treat it as exactly 2 (L2 case)\n",
    "        if abs(alpha_loss - 2) < eps:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  # ∂ψ/∂α = 0 for the L2 loss\n",
    "\n",
    "        # similarly guard α≈0\n",
    "        if abs(alpha_loss - 0) < eps:\n",
    "            # use the closed-form ∂ψ/∂α at α=0 (which you've already set to 0)\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha\n",
    "\n",
    "        # if you ever parameterize \"α = −∞\" as, say, alpha_loss < some negative cutoff\n",
    "        if alpha_loss < -1e3:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c)\n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  \n",
    "\n",
    "        # otherwise you're safely away from the singularities, so use your general formula:\n",
    "        alpha= alpha_loss\n",
    "        e2 = e_t**2\n",
    "        denom = c**2 * np.abs(alpha - 2)\n",
    "        A = (e2 / denom) + 1\n",
    "\n",
    "        # Compute the bracket term\n",
    "        term_log = np.log(A) / 2\n",
    "        term_frac = (e2 * (alpha/2 - 1)) / (c**2 * A * np.abs(alpha - 2) * (alpha - 2))\n",
    "        bracket = term_log - term_frac\n",
    "\n",
    "        # Combine everything\n",
    "        result = (e_t * A**(alpha/2 - 1) * bracket) / (c**2)\n",
    "        return result\n",
    "\n",
    "        \n",
    "    \n",
    "    def dpsi_dc(self, e_t, c, alpha_loss):\n",
    "       \n",
    "        eps = 1e-4\n",
    "\n",
    "        if alpha_loss == 2:\n",
    "            # dψ/dc = -2e / c³\n",
    "            return -2 * e_t / c**3\n",
    "\n",
    "        elif abs(alpha_loss) < eps:\n",
    "            # dψ/dc = -8ec / (e² + 2c²)²\n",
    "            return -8 * e_t * c / (e_t**2 + 2 * c**2)**2\n",
    "\n",
    "        elif alpha_loss < -1e3:\n",
    "            # dψ/dc = ( -2e / c³ + e³ / c⁵ ) * exp(-½ (e/c)²)\n",
    "            z = e_t / c\n",
    "            exp_term = np.exp(-0.5 * z**2)\n",
    "            return (-2 * e_t / c**3 + e_t**3 / c**5) * exp_term\n",
    "\n",
    "        else:\n",
    "            alpha = alpha_loss\n",
    "            e2 = e_t**2\n",
    "            denom = np.abs(alpha - 2) * c**2\n",
    "            A = e2 / denom + 1\n",
    "\n",
    "            term1 = -2 * e_t * A**(alpha/2 - 1) / (c**3)\n",
    "            term2 = -2 * e_t**3 * (alpha/2 - 1) * A**(alpha/2 - 2) / (np.abs(alpha - 2) * c**5)\n",
    "            \n",
    "            return term1 + term2\n",
    "            # General case:\n",
    "            \n",
    "    \n",
    "    def _filter_location(self, y: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        T = len(y)\n",
    "        omega, gamma, beta = params[:3]\n",
    "        \n",
    "        param_idx = 3\n",
    "        \n",
    "        # Get alpha_loss parameter\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = params[param_idx]\n",
    "            param_idx += 1\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "        \n",
    "        # Get c parameter\n",
    "        if self.c is None:\n",
    "            c = params[param_idx]\n",
    "        else:\n",
    "            c = self.c\n",
    "        \n",
    "        # Initialize location with the unconditional mean\n",
    "        f = np.zeros(T+1)\n",
    "        f[0] = omega/(1-beta)\n",
    "        \n",
    "        # Recursively update the location\n",
    "        for t in range(T):\n",
    "            # Use e(y_t, f_t) = y_t - f_t for location model\n",
    "            e_t = (y[t] - f[t])\n",
    "            psi_t = self._rho_derivative(e_t, alpha_loss, c)   #\n",
    "            f[t+1] = omega + gamma * psi_t + beta * f[t]\n",
    "        \n",
    "        return f[1:]\n",
    "    \n",
    "    def _compute_derivatives(self, y: np.ndarray, f: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "       \n",
    "        T = len(y)\n",
    "        omega, gamma, beta = params[:3]\n",
    "        \n",
    "        param_idx = 3\n",
    "        n_params = 3\n",
    "        \n",
    "        # Get alpha_loss parameter\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = params[param_idx]\n",
    "            param_idx += 1\n",
    "            n_params += 1\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "        \n",
    "        # Get c parameter\n",
    "        if self.c is None:\n",
    "            c = params[param_idx]\n",
    "            n_params += 1\n",
    "        else:\n",
    "            c = self.c\n",
    "        \n",
    "        # Initialize derivatives of f_t with respect to theta\n",
    "        e0 = y[0] - f[0]\n",
    "        psi0 = self._rho_derivative(e0, alpha_loss, c) * (1)\n",
    "        d2psi0_df = self._rho_second_derivative(e0, alpha_loss, c) * (-1)**2  # Chain rule\n",
    "        df_dtheta = np.zeros((T, n_params))\n",
    "        df_dtheta[0, 0] = 1                  # ∂f₁/∂ω\n",
    "        df_dtheta[0, 1] = psi0               # ∂f₁/∂γ\n",
    "        df_dtheta[0, 2] = f[0]               # ∂f₁/∂β\n",
    "        \n",
    "        # Initialize derivatives for alpha_loss and c if they are estimated\n",
    "        param_idx = 3\n",
    "        \n",
    "        if self.alpha_loss is None:\n",
    "            dpsi0_dalpha = self.dpsi_dalpha(e0, c, alpha_loss)\n",
    "            df_dtheta[0, param_idx] = gamma * dpsi0_dalpha  # ∂f₁/∂α\n",
    "            param_idx += 1\n",
    "        \n",
    "        if self.c is None:\n",
    "            dpsi0_dc = self.dpsi_dc(e0, c, alpha_loss)\n",
    "            df_dtheta[0, param_idx] = gamma * dpsi0_dc  # ∂f₁/∂c\n",
    "    \n",
    "        # According to the recursive formula:\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            e_t = (y[t-1] - f[t-1])\n",
    "            psi_t = self._rho_derivative(e_t, alpha_loss, c)   # Negative because e_t = y_t - f_t\n",
    "            d2psi_df = self._rho_second_derivative(e_t, alpha_loss, c)  # Chain rule\n",
    "            \n",
    "            # Common term in recursive updates\n",
    "            common_term = gamma * d2psi_df + beta\n",
    "            \n",
    "            # Derivative with respect to parameters\n",
    "            # For omega (∂ω/∂θ_0 = 1, else 0)\n",
    "            df_dtheta[t, 0] = 1 + df_dtheta[t-1, 0] * common_term\n",
    "            \n",
    "            # For gamma (∂γ/∂θ_1 = 1, else 0)\n",
    "            df_dtheta[t, 1] = psi_t + df_dtheta[t-1, 1] * common_term\n",
    "            \n",
    "            # For beta (∂β/∂θ_2 = 1, else 0)\n",
    "            df_dtheta[t, 2] = f[t-1] + df_dtheta[t-1, 2] * common_term\n",
    "            \n",
    "            # Reset param_idx for additional parameters\n",
    "            param_idx = 3\n",
    "            \n",
    "            # Derivative with respect to alpha_loss, if applicable\n",
    "            if self.alpha_loss is None:\n",
    "                dpsi_dalpha = self.dpsi_dalpha(e_t, c, alpha_loss)\n",
    "                df_dtheta[t, param_idx] = gamma * dpsi_dalpha + common_term * df_dtheta[t-1, param_idx]\n",
    "                param_idx += 1\n",
    "            \n",
    "            # Derivative with respect to c, if applicable\n",
    "            if self.c is None:\n",
    "                dpsi_dc = self.dpsi_dc(e_t, c, alpha_loss)\n",
    "                df_dtheta[t, param_idx] = gamma * dpsi_dc + common_term * df_dtheta[t-1, param_idx]\n",
    "        \n",
    "        return df_dtheta\n",
    "    #def _qle_objective(self, params: np.ndarray, y_input) -> float:\n",
    "        # 1) Dwing y_input om naar numpy‐array\n",
    "        if isinstance(y_input, pd.Series):\n",
    "            y = y_input.values\n",
    "        else:\n",
    "            y = y_input\n",
    "        \n",
    "        # 2) Haal parameters (omega, gamma, beta, [alpha], [c]) uit params\n",
    "        #    Zorg direct dat c > 1e-6:\n",
    "        omega, gamma, beta = params[:3]\n",
    "        idx = 3\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = params[idx]; idx += 1\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "        if self.c is None:\n",
    "            c = params[idx]\n",
    "        else:\n",
    "            c = self.c\n",
    "\n",
    "        # 3) Striktere guard‐checks vóóraleer rekent te starten\n",
    "        #    (a) c > 1e-6\n",
    "        if c <= 1e-6:\n",
    "            return 1e10\n",
    "        #    (b) |alpha − 2| ≥ 1e-4, tenzij je de special‐case wilt forceren\n",
    "        if self.alpha_loss is None:\n",
    "            if abs(alpha_loss - 2.0) < 1e-4:\n",
    "                # óf forceer tiny verplaatsing:\n",
    "                alpha_loss = 2.0 + 1e-4\n",
    "            if abs(alpha_loss - 0.0) < 1e-4:\n",
    "                alpha_loss = 0.0 + 1e-4\n",
    "        #    (c) Beta‐ en Gamma‐constraints:\n",
    "        if beta < 0.5 or beta >= 1.0:\n",
    "            return 1e10\n",
    "        if gamma < 0.0:\n",
    "            return 1e10\n",
    "        if gamma + beta >= 0.999:\n",
    "            return 1e10\n",
    "\n",
    "        # 4) Bereken de gefilterde locatie f (NumPy‐array) met deze parameters\n",
    "        try:\n",
    "            f = self._filter_location(y, np.array([omega, gamma, beta] + \n",
    "                ([alpha_loss] if self.alpha_loss is None else []) +\n",
    "                ([c]         if self.c is None else [])))\n",
    "        except Exception:\n",
    "            # Als filter al een fout geeft (bv delta overflow in psi), geef penalty\n",
    "            return 1e10\n",
    "\n",
    "        # 5) Reken de residuals en df/dtheta\n",
    "        h_t = y - f\n",
    "        sigma2_t = np.ones_like(f)  # (constante variantie in location‐model)\n",
    "\n",
    "        # 6) Bereken afgeleiden df/dtheta; zorg dat die geen nan produceert\n",
    "        try:\n",
    "            df_dtheta = self._compute_derivatives(y, f, np.array([omega, gamma, beta] +\n",
    "                                ([alpha_loss] if self.alpha_loss is None else []) +\n",
    "                                ([c]         if self.c is None else [])))\n",
    "        except Exception:\n",
    "            return 1e10\n",
    "\n",
    "        # 7) Test of er ergens in df_dtheta nan zit\n",
    "        if np.isnan(df_dtheta).any():\n",
    "            return 1e10\n",
    "\n",
    "        # 8) Bereken G_t en objectief\n",
    "        #    Zorg dat h_t en sigma2_t NumPy‐arrays zijn (dat hebben we al via y)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            ratio = (h_t.reshape(-1,1) / sigma2_t.reshape(-1,1))\n",
    "            # Als ratio of df_dtheta ooit NaN of Inf geeft, pennen we het ruimschoots\n",
    "            if np.isnan(ratio).any():\n",
    "                return 1e10\n",
    "            G_t = np.sum(ratio * df_dtheta, axis=0) / len(y)\n",
    "        # 9) Met normaals:\n",
    "        if np.isnan(G_t).any():\n",
    "            return 1e10\n",
    "\n",
    "        obj = np.linalg.norm(G_t)\n",
    "        if np.isnan(obj) or np.isinf(obj):\n",
    "            return 1e10\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def _qle_objective(self, params: np.ndarray, y: np.ndarray) -> float:\n",
    "        \n",
    "        try:\n",
    "            # Apply parameter constraints\n",
    "            param_idx = 3\n",
    "            \n",
    "            if self.alpha_loss is None:\n",
    "                # Ensure alpha_loss is in reasonable range\n",
    "                if params[param_idx] < -5 or params[param_idx] > 10:\n",
    "                    return 1e10\n",
    "                param_idx += 1\n",
    "            \n",
    "            if self.c is None:\n",
    "                # Ensure c is positive and reasonable\n",
    "                if params[param_idx] <= 0 or params[param_idx] > 2:\n",
    "                    return 1e10\n",
    "            \n",
    "            # Basic parameter constraints for location model stability\n",
    "            if  params[1] < 0 or params[2] < 0.5 or params[2] >= 1 or params[1] + params[2] >= 1:\n",
    "                return 1e10\n",
    "            \n",
    "            # Filter location\n",
    "            f = self._filter_location(y, params)\n",
    "            \n",
    "            # Compute residuals - h_t is defined as y_t - f_t(θ)\n",
    "            h_t = y - f\n",
    "            \n",
    "            # Use constant variance for the location model\n",
    "            sigma2_t = np.ones_like(f)\n",
    "            \n",
    "            # Compute derivatives of f_t with respect to parameters\n",
    "            df_dtheta = self._compute_derivatives(y, f, params)\n",
    "            \n",
    "            # G_t(θ) = (1/T) * sum[ h_t(θ) / σ²_t(θ) * ∂f_t(θ)/∂θ ]\n",
    "            G_t = np.sum(h_t.reshape(-1, 1) / sigma2_t.reshape(-1, 1) * df_dtheta, axis=0) / len(y)\n",
    "            \n",
    "            # The objective is to minimize ||G_t(θ)||²\n",
    "            obj = np.linalg.norm(G_t)\n",
    "            \n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {e}\")\n",
    "            return 1e10\n",
    "    \n",
    "    def fit(self, y: np.ndarray, initial_params: Optional[Dict] = None, \n",
    "            method: str = 'Nelder-Mead', maxiter: int = 2000) -> Dict:\n",
    "        \n",
    "        # Set default initial parameters if not provided\n",
    "        if initial_params is None:\n",
    "            # Use sample mean for omega initialization\n",
    "            mean_y = np.mean(y)\n",
    "            initial_params = {\n",
    "                'omega': mean_y*0.2,  # Start with a fraction of the mean\n",
    "                'gamma': 0.15, \n",
    "                'beta': 0.75\n",
    "            }\n",
    "            if self.alpha_loss is None:\n",
    "                initial_params['alpha_loss'] = 2  # Default value close to Cauchy loss\n",
    "            \n",
    "            if self.c is None:\n",
    "                initial_params['c'] = 0.5  # Use standard deviation for scale parameter\n",
    "        \n",
    "        # Prepare initial parameter array\n",
    "        init_params = np.array([initial_params[name] for name in self.param_names])\n",
    "        \n",
    "        # Run optimization\n",
    "        options = {'maxiter': maxiter, 'disp': True}\n",
    "        \n",
    "        # Different optimization methods may work better in different cases\n",
    "        if method == 'Nelder-Mead':\n",
    "            options['adaptive'] = True  # Use adaptive Nelder-Mead for better convergence\n",
    "            result = minimize(\n",
    "                self._qle_objective, \n",
    "                init_params, \n",
    "                args=(y,), \n",
    "                method=method, \n",
    "                options=options,\n",
    "            )\n",
    "        elif method == 'BFGS':\n",
    "            options = {'maxiter': maxiter, 'gtol': 1e-6}\n",
    "            result = minimize(\n",
    "                self._qle_objective, \n",
    "                init_params, \n",
    "                args=(y,), \n",
    "                method=method, \n",
    "                options=options,\n",
    "            )\n",
    "        elif method == 'differential_evolution':\n",
    "            # Set up bounds for differential evolution\n",
    "            bounds = []\n",
    "            \n",
    "            # Basic parameters bounds - adjusted for location model\n",
    "            bounds.extend([(-2, 2), (0.001, 0.5), (0.6, 0.999)])\n",
    "            \n",
    "            # Alpha bounds if estimated\n",
    "            if self.alpha_loss is None:\n",
    "                bounds.append((-5, 5))\n",
    "            \n",
    "            # c bounds if estimated\n",
    "            if self.c is None:\n",
    "                bounds.append((0.1, 5.0))\n",
    "            \n",
    "            result = differential_evolution(\n",
    "                self._qle_objective,\n",
    "                bounds=bounds,\n",
    "                args=(y,),\n",
    "                strategy='best1bin',\n",
    "                maxiter=maxiter,\n",
    "                disp=True,\n",
    "                polish=True\n",
    "            )\n",
    "        \n",
    "        if not result.success and method != 'differential_evolution':\n",
    "            print(f\"Warning: Optimization did not converge: {result.message}\")\n",
    "            \n",
    "            # Try again with different method if first one failed\n",
    "            if method == 'Nelder-Mead':\n",
    "                print(\"Trying BFGS method instead...\")\n",
    "                result = minimize(\n",
    "                    self._qle_objective,\n",
    "                    init_params,\n",
    "                    args=(y,),\n",
    "                    method='BFGS',\n",
    "                    options={'maxiter': maxiter}\n",
    "                )\n",
    "        \n",
    "        # Store parameters\n",
    "        self.params = {name: val for name, val in zip(self.param_names, result.x)}\n",
    "        \n",
    "        # Compute fitted location\n",
    "        param_array = np.array([self.params[name] for name in self.param_names])\n",
    "        self.fitted_location = self._filter_location(y, param_array)\n",
    "        self.residuals = y - self.fitted_location\n",
    "        \n",
    "        print(f\"Optimization result: {result.message}\")\n",
    "        print(f\"Parameters: {self.params}\")\n",
    "        \n",
    "        return self.params\n",
    "    #def fit(self, y: np.ndarray, initial_params: Optional[Dict] = None, \n",
    "        #method: str = 'Nelder-Mead', maxiter: int = 2000) -> Dict:\n",
    "        \"\"\"\n",
    "        Pas hier de initialisatie aan door AR(1)-momentenschatter op y te gebruiken.\n",
    "        \"\"\"\n",
    "        T = len(y)\n",
    "        y_mean = np.mean(y)\n",
    "        \n",
    "        # 1) OLS-AR(1): bereken beta_hat en omega_hat\n",
    "        #    (we centreren eerst op het gemiddelde voor stabiliteit):\n",
    "        y_centered = y - y_mean\n",
    "        num = np.sum(y_centered[1:] * y_centered[:-1])\n",
    "        den = np.sum(y_centered[:-1]**2)\n",
    "        beta_hat = num / den\n",
    "        \n",
    "        \n",
    "        # 2) Omega bij stationariteit: omega_hat = y_mean * (1 - beta_hat)\n",
    "        omega_hat = y_mean * (1 - beta_hat)\n",
    "        \n",
    "        # 3) Filter f^(0) met AR(1) om voorlopige residuen e^(0) te krijgen\n",
    "        f0 = np.zeros(T)\n",
    "        f0[0] = y_mean  # startwaarde\n",
    "        for t in range(1, T):\n",
    "            f0[t] = omega_hat + beta_hat * f0[t-1]\n",
    "        e0 = y - f0\n",
    "        \n",
    "        # 4) Bereken residu-variantie en MAD(e0)\n",
    "        sigma_u_hat = np.sqrt(np.sum(e0**2) / (T-1))   # L2-variantie van residuen\n",
    "        # Voor c gebruiken we MAD:\n",
    "        median_e0 = np.median(e0)\n",
    "        mad_e0 = 1.4826 * np.median(np.abs(e0 - median_e0))\n",
    "        \n",
    "        # 5) Kies gamma_0 zodat Var(gamma * e0) ≈ sigma_u_hat^2\n",
    "        #    In de praktijk is Var(e0) ≈ sigma_u_hat^2, dus gamma0 ≈ 1.\n",
    "        gamma_hat = 0.8  # een iets conservatievere start (tussen 0.5 en 1.0)\n",
    "        \n",
    "        # 6) Kies alpha_0 en c_0 (indien zelf te optimaliseren)\n",
    "        alpha0 = 0.5     # start in Cauchy-gebied\n",
    "        c0 = 1\n",
    "\n",
    "        \n",
    "        # 7) Vul initial_params aan (als gebruiker niets doorgeeft)\n",
    "        if initial_params is None:\n",
    "            initial_params = {\n",
    "                'omega': omega_hat,\n",
    "                'gamma': gamma_hat,\n",
    "                'beta': beta_hat\n",
    "            }\n",
    "            if self.alpha_loss is None:\n",
    "                initial_params['alpha_loss'] = alpha0\n",
    "            if self.c is None:\n",
    "                initial_params['c'] = c0\n",
    "        \n",
    "        # 8) Zet de initial_params om in array volgorde volgens self.param_names\n",
    "        init_params = np.array([initial_params[name] for name in self.param_names])\n",
    "\n",
    "        print(\">>> init_params =\", init_params)\n",
    "        print(\">>> objective(init_params) =\", self._qle_objective(init_params, y))\n",
    "\n",
    "\n",
    "        \n",
    "        # 9) Start de optimalisatie (Nelder-Mead, BFGS, etc.) zoals eerder.\n",
    "        #    ...\n",
    "        options = {'maxiter': maxiter, 'disp': True}\n",
    "        if method == 'Nelder-Mead':\n",
    "            options['adaptive'] = True\n",
    "            result = minimize(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                args=(y,),\n",
    "                method='Nelder-Mead',\n",
    "                options=options\n",
    "            )\n",
    "        elif method == 'BFGS':\n",
    "            opts = {'maxiter': maxiter, 'gtol': 1e-6}\n",
    "            result = minimize(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                args=(y,),\n",
    "                method='BFGS',\n",
    "                options=opts\n",
    "            )\n",
    "        elif method == 'differential_evolution':\n",
    "            # ... voer bounds in ...\n",
    "            bounds = [(-2, 2), (0.001, 0.5), (0.6, 0.999)]\n",
    "            if self.alpha_loss is None:\n",
    "                bounds.append((-5, 5))\n",
    "            if self.c is None:\n",
    "                bounds.append((0.1, 5.0))\n",
    "            result = differential_evolution(\n",
    "                self._qle_objective,\n",
    "                bounds=bounds,\n",
    "                args=(y,),\n",
    "                strategy='best1bin',\n",
    "                maxiter=maxiter,\n",
    "                disp=True,\n",
    "                polish=True\n",
    "            )\n",
    "        # 10) Indien niet-convergerend, probeer alternatieve methode\n",
    "        if not result.success and method != 'differential_evolution':\n",
    "            print(f\"Waarschuwing: Optimizer converteerde niet: {result.message}\")\n",
    "            if method == 'Nelder-Mead':\n",
    "                print(\"Probeer BFGS in plaats van Nelder-Mead …\")\n",
    "                result = minimize(\n",
    "                    self._qle_objective,\n",
    "                    init_params,\n",
    "                    args=(y,),\n",
    "                    method='BFGS',\n",
    "                    options={'maxiter': maxiter}\n",
    "                )\n",
    "        \n",
    "        # 11) Sla resultaat op en bereken fitted_location, residuals, …\n",
    "        self.params = {name: val for name, val in zip(self.param_names, result.x)}\n",
    "        param_array = np.array([self.params[name] for name in self.param_names])\n",
    "        self.fitted_location = self._filter_location(y, param_array)\n",
    "        self.residuals = y - self.fitted_location\n",
    "        \n",
    "        print(f\"Optimalisatie resultaat: {result.message}\")\n",
    "        print(f\"Beginwaarden gebruikt: omega={omega_hat:.4f}, beta={beta_hat:.4f}, \"\n",
    "            f\"gamma={gamma_hat:.4f}, alpha={initial_params.get('alpha_loss', None)}, c={initial_params.get('c', None)}\")\n",
    "        print(f\"Gevonden parameters: {self.params}\")\n",
    "        return self.params\n",
    "    def plot_location(self, gas_est: None, date_index : None , y: np.ndarray, true_loc: np.ndarray = None, title: str = \"Estimated Location\") -> None:\n",
    "        \n",
    "        if self.fitted_location is None:\n",
    "            raise ValueError(\"Model must be fit before plotting\")\n",
    "        date_index = np.array(date_index)\n",
    "        gas_est = np.array(gas_est)\n",
    "        plt.figure(figsize=(12,6))\n",
    "        # Plot de fitted location tegen dezelfde index\n",
    "        plt.plot(date_index, self.fitted_location, 'r-', linewidth=2, label='Estimated Location')\n",
    "        # Plot de data zelf (y) tegen dezelfde index\n",
    "        plt.plot(date_index[0:300], y[0:300], 'g--', alpha=0.8, label='Transformed Data')\n",
    "        plt.plot(date_index[0:300], gas_est[0:300], 'b--', alpha=0.8, label='Gas Estimation')\n",
    "        plt.title(title)\n",
    "        plt.ylim(50,80)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "class GAS_Location_Model:\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        GAS(1,1) location model:\n",
    "        y_t = mu_t + e_t,   e_t ~ N(0, sigma^2)\n",
    "        link=f_identity (f_t = mu_t), scaling=identity\n",
    "        Model: f_{t+1} = omega + alpha * s_t + beta * f_t\n",
    "        where s_t = score of the log-likelihood w.r.t. mu_t\n",
    "        \"\"\"\n",
    "        self.y = np.asarray(y)\n",
    "        self.T = len(y)\n",
    "\n",
    "    def _update_f(self, params):\n",
    "        omega, alpha, beta = params\n",
    "        mu = np.zeros(self.T)\n",
    "        ll = 0.0\n",
    "        mu[0] = np.mean(self.y)  # initialize\n",
    "        sigma2 = np.var(self.y)  # fixed variance for simplicity\n",
    "\n",
    "        for t in range(1, self.T):\n",
    "            score_t = (self.y[t-1] - mu[t-1]) / sigma2  # dlogL/dmu_{t-1}\n",
    "            mu[t] = omega + alpha * score_t + beta * mu[t-1]\n",
    "            ll += -0.5 * (np.log(2 * np.pi * sigma2) + (self.y[t] - mu[t])**2 / sigma2)\n",
    "\n",
    "        return -ll, mu\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"Estimate GAS location model parameters by ML\"\"\"\n",
    "        maxiter = 1000\n",
    "        verbose = True\n",
    "\n",
    "        initial_params = {\n",
    "            'omega': 0.0,\n",
    "            'alpha': 0.1,\n",
    "            'beta': 0.8\n",
    "        }\n",
    "\n",
    "        init_params = np.array([initial_params[name] for name in initial_params])\n",
    "\n",
    "        options = {'maxiter': maxiter, 'disp': verbose, 'adaptive': True}\n",
    "        res = minimize(\n",
    "            lambda params, y: self._update_f(params)[0],\n",
    "            init_params,\n",
    "            args=(self.y,),\n",
    "            method='Nelder-Mead',\n",
    "            options=options\n",
    "        )\n",
    "        self.params = res.x\n",
    "        _, self.fitted_mu = self._update_f(self.params)\n",
    "        return res\n",
    "\n",
    "    def get_fitted_location(self):\n",
    "        return self.fitted_mu\n",
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "from scipy.optimize import minimize\n",
    "class GAS_t_Location_Model:\n",
    "    def __init__(self, y):\n",
    "        self.y = np.asarray(y)\n",
    "        self.T = len(y)\n",
    "\n",
    "    def _loglik_and_update(self, params):\n",
    "        omega, phi, kappa, lambda_, nu = params\n",
    "        mu = np.zeros(self.T)\n",
    "        ll = 0.0\n",
    "\n",
    "        # Initialize\n",
    "        mu[0] = np.mean(self.y)\n",
    "\n",
    "        for t in range(1, self.T):\n",
    "            yt1 = self.y[t - 1]\n",
    "            mut1 = mu[t - 1]\n",
    "\n",
    "            # Score of Student's t log-likelihood w.r.t. location\n",
    "            scale = np.exp(lambda_)\n",
    "            standardized_residual = (yt1 - mut1) / scale\n",
    "            u_t = ((nu + 1) * standardized_residual) / (nu + standardized_residual**2)\n",
    "\n",
    "            # Update location (mu)\n",
    "            mu[t] = omega + phi * mut1 + kappa * u_t\n",
    "\n",
    "            # Log-likelihood contribution of Student's t\n",
    "            yt = self.y[t]\n",
    "            sres = (yt - mu[t]) / scale\n",
    "            ll += gammaln((nu + 1) / 2) - gammaln(nu / 2) - 0.5 * np.log(np.pi * nu) \\\n",
    "                  - lambda_ - 0.5 * (nu + 1) * np.log(1 + (sres ** 2) / nu)\n",
    "\n",
    "        return -ll, mu\n",
    "\n",
    "    def fit(self):\n",
    "        init_params = np.array([0.0, 0.9, 0.8, 0.0, 6.0])  # [omega, phi, kappa, lambda, nu]\n",
    "        bounds = [(-np.inf, np.inf), (-0.999, 0.999), (0.01, 2.0), (-5, 5), (2.1, 100)]\n",
    "\n",
    "        result = minimize(\n",
    "            lambda p: self._loglik_and_update(p)[0],\n",
    "            init_params,\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'disp': True}\n",
    "        )\n",
    "        self.params = result.x\n",
    "        _, self.mu_fitted = self._loglik_and_update(self.params)\n",
    "        return result\n",
    "\n",
    "    def get_fitted_location(self):\n",
    "        return self.mu_fitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4724a100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, price_area='DK2'):\n",
    "    \"\"\"\n",
    "    Load and preprocess electricity spot price data by splitting each day into \n",
    "    four quartiles and calculating the mean price for each quartile.\n",
    "    \n",
    "    This approach provides 4 observations per day instead of weekly Monday averages,\n",
    "    capturing intraday price dynamics while maintaining temporal aggregation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing Nord Pool spot prices\n",
    "    price_area : str\n",
    "        Price area to filter for (e.g., 'DK1', 'DK2')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    y : numpy.ndarray\n",
    "        Quartile time-series constructed by taking the mean electricity spot price \n",
    "        for each quartile of the day, taking logs and multiplying by 10\n",
    "    quartile_prices : pandas.Series\n",
    "        Quartile mean spot prices time series (for analysis)\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    print(f\"Loading Nord Pool electricity spot price data from: {file_path}\")\n",
    "    print(\"Methodology: Daily quartile time-series by taking mean spot price for each 6-hour period\")\n",
    "    \n",
    "    # Load the dataset with proper separator\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "        print(f\"Date range: {df['HourDK'].min()} to {df['HourDK'].max()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Fix datetime parsing\n",
    "    df['HourDK'] = pd.to_datetime(df['HourDK'], errors='coerce')\n",
    "    \n",
    "    # Fix numeric columns - handle European decimal format (comma as decimal separator)\n",
    "    print(\"Converting price columns to numeric...\")\n",
    "    \n",
    "    def convert_european_decimal(series):\n",
    "        \"\"\"Convert European decimal format (comma as decimal separator) to float\"\"\"\n",
    "        if series.dtype == 'object':\n",
    "            # Replace comma with dot for decimal separator\n",
    "            return pd.to_numeric(series.astype(str).str.replace(',', '.'), errors='coerce')\n",
    "        else:\n",
    "            return series\n",
    "    \n",
    "    # Convert price columns\n",
    "    df['SpotPriceDKK'] = convert_european_decimal(df['SpotPriceDKK'])\n",
    "    df['SpotPriceEUR'] = convert_european_decimal(df['SpotPriceEUR'])\n",
    "    \n",
    "    # Check for conversion issues\n",
    "    dkk_na = df['SpotPriceDKK'].isna().sum()\n",
    "    eur_na = df['SpotPriceEUR'].isna().sum()\n",
    "    \n",
    "    print(f\"Price conversion complete. DKK NAs: {dkk_na}, EUR NAs: {eur_na}\")\n",
    "    \n",
    "    if dkk_na > 0:\n",
    "        print(f\"Warning: {dkk_na} DKK prices could not be converted\")\n",
    "    if eur_na > 0:\n",
    "        print(f\"Warning: {eur_na} EUR prices could not be converted\")\n",
    "    \n",
    "    # Check for parsing errors\n",
    "    if df['HourDK'].isna().any():\n",
    "        print(f\"Warning: {df['HourDK'].isna().sum()} datetime parsing errors found\")\n",
    "        df = df.dropna(subset=['HourDK'])\n",
    "    \n",
    "    # Remove rows with invalid price data\n",
    "    df = df.dropna(subset=['SpotPriceDKK'])\n",
    "    \n",
    "    # Filter for specific price area\n",
    "    if price_area:\n",
    "        available_areas = df['PriceArea'].unique()\n",
    "        print(f\"Available price areas: {available_areas}\")\n",
    "        \n",
    "        if price_area not in available_areas:\n",
    "            print(f\"Warning: {price_area} not found. Using first available area: {available_areas[0]}\")\n",
    "            price_area = available_areas[0]\n",
    "        \n",
    "        df = df[df['PriceArea'] == price_area]\n",
    "        print(f\"Filtered for {price_area}. Remaining rows: {len(df)}\")\n",
    "    \n",
    "    # Set datetime as index and filter date range\n",
    "    df = df.set_index('HourDK')\n",
    "    df = df[df.index > '2016-09-01']\n",
    "    df = df[df.index < '2017-09-01']\n",
    "    print(f\"Date filtered data. Remaining rows: {len(df)}\")\n",
    "    \n",
    "    # Create date and hour columns for quartile calculation\n",
    "    df['Date'] = df.index.date\n",
    "    df['Hour'] = df.index.hour\n",
    "    \n",
    "    # Define quartiles based on hour of day (0-23)\n",
    "    # Quartile 1: 00:00-05:59 (hours 0-5)    - Night/Early Morning\n",
    "    # Quartile 2: 06:00-11:59 (hours 6-11)   - Morning\n",
    "    # Quartile 3: 12:00-17:59 (hours 12-17)  - Afternoon\n",
    "    # Quartile 4: 18:00-23:59 (hours 18-23)  - Evening/Night\n",
    "    \n",
    "    def assign_quartile(hour):\n",
    "        if 0 <= hour <= 5:\n",
    "            return 1\n",
    "        elif 6 <= hour <= 11:\n",
    "            return 2\n",
    "        elif 12 <= hour <= 17:\n",
    "            return 3\n",
    "        else:  # 18 <= hour <= 23\n",
    "            return 4\n",
    "    \n",
    "    df['Quartile'] = df['Hour'].apply(assign_quartile)\n",
    "    \n",
    "    print(\"Daily quartile assignment:\")\n",
    "    print(\"Quartile 1: 00:00-05:59 (Night/Early Morning)\")\n",
    "    print(\"Quartile 2: 06:00-11:59 (Morning)\")\n",
    "    print(\"Quartile 3: 12:00-17:59 (Afternoon)\")\n",
    "    print(\"Quartile 4: 18:00-23:59 (Evening/Night)\")\n",
    "    \n",
    "    # Calculate quartile means for each day\n",
    "    quartile_data = df.groupby(['Date', 'Quartile'])['SpotPriceDKK'].mean().reset_index()\n",
    "    \n",
    "    # Create a proper datetime index for each quartile\n",
    "    # We'll use the middle hour of each quartile as representative time\n",
    "    quartile_hours = {1: 3, 2: 9, 3: 15, 4: 21}  # Representative hours for each quartile\n",
    "    \n",
    "    quartile_data['DateTime'] = pd.to_datetime(quartile_data['Date']) + \\\n",
    "                               pd.to_timedelta(quartile_data['Quartile'].map(quartile_hours), unit='h')\n",
    "    \n",
    "    # Set datetime as index and sort\n",
    "    quartile_data = quartile_data.set_index('DateTime').sort_index()\n",
    "    \n",
    "    # Extract the price series\n",
    "    quartile_prices = quartile_data['SpotPriceDKK']\n",
    "    \n",
    "    print(f\"Quartile observations: {len(quartile_prices)} (should be ~4x daily observations)\")\n",
    "    print(f\"Quartile price statistics - Min: {quartile_prices.min():.2f} DKK, Max: {quartile_prices.max():.2f} DKK, Mean: {quartile_prices.mean():.2f} DKK\")\n",
    "    \n",
    "    # Check quartile distribution\n",
    "    quartile_counts = quartile_data['Quartile'].value_counts().sort_index()\n",
    "    print(\"Quartile distribution:\")\n",
    "    for q in [1, 2, 3, 4]:\n",
    "        count = quartile_counts.get(q, 0)\n",
    "        print(f\"  Quartile {q}: {count} observations\")\n",
    "    \n",
    "    # Remove any remaining NaN values\n",
    "    quartile_prices = quartile_prices.dropna()\n",
    "    print(f\"After removing NaN: {len(quartile_prices)} quartile observations\")\n",
    "    \n",
    "    if len(quartile_prices) == 0:\n",
    "        print(\"Error: No valid quartile prices after preprocessing\")\n",
    "        return None, None\n",
    "    \n",
    "    # Handle negative or zero prices (if any)\n",
    "    if (quartile_prices <= 0).any():\n",
    "        print(\"Warning: Non-positive prices found. Adding small constant before log transform\")\n",
    "        quartile_prices = quartile_prices + abs(quartile_prices.min()) + 1\n",
    "    \n",
    "    # Apply transformation: take logs and multiply by 10\n",
    "    log_prices = np.log(quartile_prices) * 10\n",
    "    y_log = np.log(quartile_prices.values) * 10\n",
    "\n",
    "    \n",
    "    # Final time series\n",
    "    y = y_log\n",
    "    \n",
    "    print(f\"\\nTransformation complete with quartile methodology:\")\n",
    "    print(f\"- Daily time-series constructed by splitting each day into 4 quartiles (6-hour periods)\")\n",
    "    print(f\"- Taking mean electricity spot price for each quartile\")\n",
    "    print(f\"- Taking logs of quartile means and multiplying by 10\")\n",
    "    print(f\"- Centering around mean for stability\")\n",
    "    print(f\"Final array shape: {y.shape}\")\n",
    "    print(f\"Transformed price statistics - Min: {y.min():.2f}, Max: {y.max():.2f}, Mean: {y.mean():.2f}\")\n",
    "    \n",
    "    # Add quartile information to the series for later analysis\n",
    "    quartile_prices_with_info = quartile_data[['SpotPriceDKK', 'Quartile']].copy()\n",
    "    quartile_prices_with_info['LogTransformed'] = y\n",
    "    \n",
    "    return y, quartile_prices\n",
    "\n",
    "\n",
    "def analyze_quartile_patterns(quartile_data):\n",
    "    \"\"\"\n",
    "    Analyze price patterns across different quartiles of the day\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    quartile_data : pandas.DataFrame\n",
    "        DataFrame containing quartile price data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    analysis : dict\n",
    "        Dictionary containing quartile analysis results\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # Group by quartile to analyze patterns\n",
    "    quartile_stats = quartile_data.groupby('Quartile')['SpotPriceDKK'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    print(\"\\nQuartile Price Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Quartile 1 (00:00-05:59): Night/Early Morning\")\n",
    "    print(\"Quartile 2 (06:00-11:59): Morning\") \n",
    "    print(\"Quartile 3 (12:00-17:59): Afternoon\")\n",
    "    print(\"Quartile 4 (18:00-23:59): Evening/Night\")\n",
    "    print(\"-\" * 60)\n",
    "    print(quartile_stats)\n",
    "    \n",
    "    # Calculate relative differences\n",
    "    overall_mean = quartile_data['SpotPriceDKK'].mean()\n",
    "    \n",
    "    print(f\"\\nRelative to Overall Mean ({overall_mean:.2f} DKK):\")\n",
    "    print(\"-\" * 40)\n",
    "    for q in [1, 2, 3, 4]:\n",
    "        q_mean = quartile_stats.loc[q, 'mean']\n",
    "        diff_pct = ((q_mean - overall_mean) / overall_mean) * 100\n",
    "        print(f\"Quartile {q}: {diff_pct:+.1f}% ({q_mean:.2f} DKK)\")\n",
    "    \n",
    "    return quartile_stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_with_quartiles():\n",
    "    \"\"\"\n",
    "    Main execution function using quartile-based preprocessing\n",
    "    \"\"\"\n",
    "    # File path\n",
    "    file_path = \"/Users/MathijsDijkstra/Downloads/Elspotprices.csv\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"EMPIRICAL ILLUSTRATION: Nord Pool Electricity Spot Prices\")\n",
    "    print(\"QUARTILE-BASED ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Analyzing quartile electricity spot prices - 4 observations per day\")\n",
    "    print(\"Each day split into 6-hour periods for enhanced temporal resolution\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # Load and preprocess data with quartile approach\n",
    "    y, quartile_prices = load_and_preprocess_data(file_path, price_area='DK2')\n",
    "    \n",
    "    if y is None:\n",
    "        print(\"Failed to load and preprocess data. Exiting.\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Additional quartile analysis if we have the detailed data\n",
    "    try:\n",
    "        # Recreate quartile analysis data for pattern analysis\n",
    "        print(\"\\nPerforming quartile pattern analysis...\")\n",
    "        # This would require the detailed quartile_data from the function\n",
    "        # For now, we'll show basic statistics\n",
    "        \n",
    "        print(f\"\\nBasic Time Series Statistics:\")\n",
    "        print(f\"Total observations: {len(y)}\")\n",
    "        print(f\"Expected daily observations: ~4 (one per quartile)\")\n",
    "        print(f\"Approximate days covered: {len(y) // 4}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Quartile analysis error: {e}\")\n",
    "    \n",
    "    print(\"-\"*80)\n",
    "    print(\"QUARTILE DATA PREPROCESSING COMPLETE\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"Ready for model fitting with enhanced temporal resolution\")\n",
    "    print(f\"Time series length: {len(y)} quartile observations\")\n",
    "    \n",
    "    return y, quartile_prices, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a47c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ROLLING WINDOW FORECASTING COMPARISON\n",
      "================================================================================\n",
      "================================================================================\n",
      "EMPIRICAL ILLUSTRATION: Nord Pool Electricity Spot Prices\n",
      "QUARTILE-BASED ANALYSIS\n",
      "================================================================================\n",
      "Analyzing quartile electricity spot prices - 4 observations per day\n",
      "Each day split into 6-hour periods for enhanced temporal resolution\n",
      "--------------------------------------------------------------------------------\n",
      "Loading Nord Pool electricity spot price data from: /Users/MathijsDijkstra/Downloads/Elspotprices.csv\n",
      "Methodology: Daily quartile time-series by taking mean spot price for each 6-hour period\n",
      "Data loaded successfully. Shape: (50831, 5)\n",
      "Date range: 2017-01-01 00:00 to 2022-10-19 23:00\n",
      "Converting price columns to numeric...\n",
      "Price conversion complete. DKK NAs: 0, EUR NAs: 0\n",
      "Available price areas: ['DK2']\n",
      "Filtered for DK2. Remaining rows: 50831\n",
      "Date filtered data. Remaining rows: 5831\n",
      "Daily quartile assignment:\n",
      "Quartile 1: 00:00-05:59 (Night/Early Morning)\n",
      "Quartile 2: 06:00-11:59 (Morning)\n",
      "Quartile 3: 12:00-17:59 (Afternoon)\n",
      "Quartile 4: 18:00-23:59 (Evening/Night)\n",
      "Quartile observations: 972 (should be ~4x daily observations)\n",
      "Quartile price statistics - Min: -63.23 DKK, Max: 681.37 DKK, Mean: 232.14 DKK\n",
      "Quartile distribution:\n",
      "  Quartile 1: 243 observations\n",
      "  Quartile 2: 243 observations\n",
      "  Quartile 3: 243 observations\n",
      "  Quartile 4: 243 observations\n",
      "After removing NaN: 972 quartile observations\n",
      "Warning: Non-positive prices found. Adding small constant before log transform\n",
      "\n",
      "Transformation complete with quartile methodology:\n",
      "- Daily time-series constructed by splitting each day into 4 quartiles (6-hour periods)\n",
      "- Taking mean electricity spot price for each quartile\n",
      "- Taking logs of quartile means and multiplying by 10\n",
      "- Centering around mean for stability\n",
      "Final array shape: (972,)\n",
      "Transformed price statistics - Min: 0.00, Max: 66.14, Mean: 56.63\n",
      "\n",
      "Performing quartile pattern analysis...\n",
      "\n",
      "Basic Time Series Statistics:\n",
      "Total observations: 972\n",
      "Expected daily observations: ~4 (one per quartile)\n",
      "Approximate days covered: 243\n",
      "--------------------------------------------------------------------------------\n",
      "QUARTILE DATA PREPROCESSING COMPLETE\n",
      "--------------------------------------------------------------------------------\n",
      "Ready for model fitting with enhanced temporal resolution\n",
      "Time series length: 972 quartile observations\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "start_idx too large - not enough data for forecasting",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 464\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     forecaster, results = \u001b[43mmain_rolling_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 452\u001b[39m, in \u001b[36mmain_rolling_forecast\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Run rolling forecast\u001b[39;00m\n\u001b[32m    451\u001b[39m start_idx = window_size + \u001b[32m100\u001b[39m  \u001b[38;5;66;03m# Leave some buffer for initial estimation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m results = \u001b[43mforecaster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_rolling_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# Print performance summary\u001b[39;00m\n\u001b[32m    455\u001b[39m forecaster.print_performance_summary()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 182\u001b[39m, in \u001b[36mRollingWindowForecaster.run_rolling_forecast\u001b[39m\u001b[34m(self, y, start_idx)\u001b[39m\n\u001b[32m    179\u001b[39m     start_idx = \u001b[38;5;28mself\u001b[39m.window_size\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_idx >= T - \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstart_idx too large - not enough data for forecasting\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Initialize storage\u001b[39;00m\n\u001b[32m    185\u001b[39m n_forecasts = T - start_idx\n",
      "\u001b[31mValueError\u001b[39m: start_idx too large - not enough data for forecasting"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RollingWindowForecaster:\n",
    "    \"\"\"\n",
    "    Rolling window forecasting framework for comparing one-step-ahead forecasts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models: Dict, window_size: int = 500):\n",
    "        \"\"\"\n",
    "        Initialize the rolling window forecaster\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        models : dict\n",
    "            Dictionary of model classes to compare\n",
    "        window_size : int\n",
    "            Size of the rolling estimation window\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.window_size = window_size\n",
    "        self.forecasts = {}\n",
    "        self.actual_values = []\n",
    "        self.forecast_errors = {}\n",
    "        self.model_params_history = {}\n",
    "        \n",
    "    def _fit_model(self, model_name: str, model_class, y_window: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit a single model to the window data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_name : str\n",
    "            Name of the model\n",
    "        model_class : class\n",
    "            Model class to instantiate and fit\n",
    "        y_window : np.ndarray\n",
    "            Data window for estimation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        fitted_model : object\n",
    "            Fitted model instance\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if model_name == 'RobustQLE':\n",
    "                model = model_class()\n",
    "                model.fit(y_window)\n",
    "                return model\n",
    "            elif model_name == 'GAS_Location':\n",
    "                model = model_class(y_window)\n",
    "                model.fit()\n",
    "                return model\n",
    "            elif model_name == 'GAS_t_Location':\n",
    "                model = model_class(y_window)\n",
    "                model.fit()\n",
    "                return model\n",
    "            else:\n",
    "                # Generic fitting approach\n",
    "                model = model_class(y_window)\n",
    "                model.fit()\n",
    "                return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _generate_forecast(self, model_name: str, fitted_model, y_window: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Generate one-step-ahead forecast from fitted model\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_name : str\n",
    "            Name of the model\n",
    "        fitted_model : object\n",
    "            Fitted model instance\n",
    "        y_window : np.ndarray\n",
    "            Data window used for estimation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        forecast : float\n",
    "            One-step-ahead forecast\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if model_name == 'RobustQLE':\n",
    "                # For RobustQLE, forecast next location value\n",
    "                if fitted_model.params is not None:\n",
    "                    omega = fitted_model.params['omega']\n",
    "                    gamma = fitted_model.params['gamma'] \n",
    "                    beta = fitted_model.params['beta']\n",
    "                    \n",
    "                    # Get current location estimate\n",
    "                    current_f = fitted_model.fitted_location[-1]\n",
    "                    \n",
    "                    # Get current residual for score computation\n",
    "                    current_e = y_window[-1] - current_f\n",
    "                    \n",
    "                    # Get loss function parameters\n",
    "                    alpha_loss = fitted_model.params.get('alpha_loss', fitted_model.alpha_loss)\n",
    "                    c = fitted_model.params.get('c', fitted_model.c)\n",
    "                    \n",
    "                    if alpha_loss is None:\n",
    "                        alpha_loss = 2.0  # Default to L2\n",
    "                    if c is None:\n",
    "                        c = 0.5  # Default scale\n",
    "                    \n",
    "                    # Compute score (derivative of rho function)\n",
    "                    psi_t = fitted_model._rho_derivative(current_e, alpha_loss, c)\n",
    "                    \n",
    "                    # Forecast next location\n",
    "                    forecast = omega + gamma * psi_t + beta * current_f\n",
    "                    return forecast\n",
    "                else:\n",
    "                    return y_window[-1]  # Fallback to last observation\n",
    "                    \n",
    "            elif model_name in ['GAS_Location', 'GAS_t_Location']:\n",
    "                # For GAS models, forecast next mu value\n",
    "                mu_fitted = fitted_model.get_fitted_location()\n",
    "                \n",
    "                if hasattr(fitted_model, 'params') and fitted_model.params is not None:\n",
    "                    params = fitted_model.params\n",
    "                    \n",
    "                    if model_name == 'GAS_Location':\n",
    "                        omega, alpha, beta = params\n",
    "                        current_mu = mu_fitted[-1]\n",
    "                        sigma2 = np.var(y_window)  # Fixed variance assumption\n",
    "                        score_t = (y_window[-1] - current_mu) / sigma2\n",
    "                        forecast = omega + alpha * score_t + beta * current_mu\n",
    "                        \n",
    "                    elif model_name == 'GAS_t_Location':\n",
    "                        omega, phi, kappa, lambda_, nu = params\n",
    "                        current_mu = mu_fitted[-1]\n",
    "                        scale = np.exp(lambda_)\n",
    "                        \n",
    "                        # Student's t score\n",
    "                        standardized_residual = (y_window[-1] - current_mu) / scale\n",
    "                        u_t = ((nu + 1) * standardized_residual) / (nu + standardized_residual**2)\n",
    "                        \n",
    "                        forecast = omega + phi * current_mu + kappa * u_t\n",
    "                    \n",
    "                    return forecast\n",
    "                else:\n",
    "                    return mu_fitted[-1]  # Fallback to last fitted value\n",
    "            else:\n",
    "                # Generic approach - assume model has a predict method\n",
    "                if hasattr(fitted_model, 'predict'):\n",
    "                    return fitted_model.predict(1)[0]  # One step ahead\n",
    "                else:\n",
    "                    return y_window[-1]  # Naive forecast\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating forecast for {model_name}: {e}\")\n",
    "            return y_window[-1]  # Naive fallback\n",
    "    \n",
    "    def run_rolling_forecast(self, y: np.ndarray, start_idx: int = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Run rolling window forecasting experiment\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        y : np.ndarray\n",
    "            Full time series data\n",
    "        start_idx : int\n",
    "            Starting index for out-of-sample forecasting\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            Dictionary containing forecasting results\n",
    "        \"\"\"\n",
    "        T = len(y)\n",
    "        \n",
    "        if start_idx is None:\n",
    "            start_idx = self.window_size\n",
    "        \n",
    "        if start_idx >= T - 1:\n",
    "            raise ValueError(\"start_idx too large - not enough data for forecasting\")\n",
    "        \n",
    "        # Initialize storage\n",
    "        n_forecasts = T - start_idx\n",
    "        for model_name in self.models.keys():\n",
    "            self.forecasts[model_name] = np.full(n_forecasts, np.nan)\n",
    "            self.forecast_errors[model_name] = np.full(n_forecasts, np.nan)\n",
    "            self.model_params_history[model_name] = []\n",
    "        \n",
    "        self.actual_values = np.full(n_forecasts, np.nan)\n",
    "        \n",
    "        print(f\"Starting rolling window forecasting...\")\n",
    "        print(f\"Window size: {self.window_size}\")\n",
    "        print(f\"Number of forecasts: {n_forecasts}\")\n",
    "        print(f\"Models: {list(self.models.keys())}\")\n",
    "        \n",
    "        # Rolling window loop\n",
    "        for t in range(start_idx, T):\n",
    "            forecast_idx = t - start_idx\n",
    "            \n",
    "            # Define estimation window\n",
    "            window_start = max(0, t - self.window_size)\n",
    "            window_end = t\n",
    "            y_window = y[window_start:window_end]\n",
    "            \n",
    "            # Store actual value\n",
    "            if t < T:\n",
    "                self.actual_values[forecast_idx] = y[t]\n",
    "            \n",
    "            if forecast_idx % 50 == 0:\n",
    "                print(f\"Processing forecast {forecast_idx + 1}/{n_forecasts} (t={t})\")\n",
    "            \n",
    "            # Fit each model and generate forecasts\n",
    "            for model_name, model_class in self.models.items():\n",
    "                try:\n",
    "                    # Fit model to current window\n",
    "                    fitted_model = self._fit_model(model_name, model_class, y_window)\n",
    "                    \n",
    "                    if fitted_model is not None:\n",
    "                        # Generate one-step-ahead forecast\n",
    "                        forecast = self._generate_forecast(model_name, fitted_model, y_window)\n",
    "                        self.forecasts[model_name][forecast_idx] = forecast\n",
    "                        \n",
    "                        # Calculate forecast error\n",
    "                        if t < T:\n",
    "                            error = y[t] - forecast\n",
    "                            self.forecast_errors[model_name][forecast_idx] = error\n",
    "                        \n",
    "                        # Store model parameters for analysis\n",
    "                        if hasattr(fitted_model, 'params') and fitted_model.params is not None:\n",
    "                            self.model_params_history[model_name].append(fitted_model.params.copy())\n",
    "                        else:\n",
    "                            self.model_params_history[model_name].append(None)\n",
    "                    else:\n",
    "                        print(f\"Failed to fit {model_name} at t={t}\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {model_name} at t={t}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        print(\"Rolling window forecasting completed!\")\n",
    "        return self._compile_results()\n",
    "    \n",
    "    def _compile_results(self) -> Dict:\n",
    "        \"\"\"Compile and return forecasting results\"\"\"\n",
    "        results = {\n",
    "            'forecasts': self.forecasts,\n",
    "            'actual_values': self.actual_values,\n",
    "            'forecast_errors': self.forecast_errors,\n",
    "            'model_params_history': self.model_params_history\n",
    "        }\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        results['performance_metrics'] = self.calculate_performance_metrics()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def calculate_performance_metrics(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate various forecast performance metrics\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Dictionary of performance metrics for each model\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        for model_name in self.models.keys():\n",
    "            errors = self.forecast_errors[model_name]\n",
    "            valid_errors = errors[~np.isnan(errors)]\n",
    "            \n",
    "            if len(valid_errors) > 0:\n",
    "                metrics[model_name] = {\n",
    "                    'MAE': np.mean(np.abs(valid_errors)),\n",
    "                    'RMSE': np.sqrt(np.mean(valid_errors**2)),\n",
    "                    'MAPE': np.mean(np.abs(valid_errors / self.actual_values[~np.isnan(errors)])) * 100,\n",
    "                    'Bias': np.mean(valid_errors),\n",
    "                    'Std_Error': np.std(valid_errors),\n",
    "                    'Min_Error': np.min(valid_errors),\n",
    "                    'Max_Error': np.max(valid_errors),\n",
    "                    'N_Forecasts': len(valid_errors)\n",
    "                }\n",
    "            else:\n",
    "                metrics[model_name] = None\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def plot_forecasting_results(self, plot_window: int = 200, save_plots: bool = False):\n",
    "        \"\"\"\n",
    "        Plot forecasting results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        plot_window : int\n",
    "            Number of recent forecasts to plot\n",
    "        save_plots : bool\n",
    "            Whether to save plots to files\n",
    "        \"\"\"\n",
    "        n_models = len(self.models)\n",
    "        \n",
    "        # Main comparison plot\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot 1: Forecast comparison\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plot_end = min(plot_window, len(self.actual_values))\n",
    "        plot_start = max(0, len(self.actual_values) - plot_window)\n",
    "        \n",
    "        x_axis = range(plot_start, plot_start + plot_end)\n",
    "        plt.plot(x_axis, self.actual_values[plot_start:plot_start + plot_end], \n",
    "                'k-', linewidth=2, label='Actual', alpha=0.8)\n",
    "        \n",
    "        colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "        for i, model_name in enumerate(self.models.keys()):\n",
    "            forecasts = self.forecasts[model_name][plot_start:plot_start + plot_end]\n",
    "            plt.plot(x_axis, forecasts, '--', \n",
    "                    color=colors[i % len(colors)], \n",
    "                    linewidth=1.5, label=f'{model_name} Forecast', alpha=0.7)\n",
    "        \n",
    "        plt.title(f'One-Step-Ahead Forecasts (Last {plot_end} observations)')\n",
    "        plt.xlabel('Forecast Period')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Forecast errors\n",
    "        plt.subplot(2, 2, 2)\n",
    "        for i, model_name in enumerate(self.models.keys()):\n",
    "            errors = self.forecast_errors[model_name][plot_start:plot_start + plot_end]\n",
    "            plt.plot(x_axis, errors, \n",
    "                    color=colors[i % len(colors)], \n",
    "                    linewidth=1, label=f'{model_name}', alpha=0.7)\n",
    "        \n",
    "        plt.title('Forecast Errors')\n",
    "        plt.xlabel('Forecast Period')\n",
    "        plt.ylabel('Error (Actual - Forecast)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Error distribution\n",
    "        plt.subplot(2, 2, 3)\n",
    "        for i, model_name in enumerate(self.models.keys()):\n",
    "            errors = self.forecast_errors[model_name]\n",
    "            valid_errors = errors[~np.isnan(errors)]\n",
    "            if len(valid_errors) > 0:\n",
    "                plt.hist(valid_errors, bins=30, alpha=0.6, \n",
    "                        color=colors[i % len(colors)], label=model_name, density=True)\n",
    "        \n",
    "        plt.title('Forecast Error Distributions')\n",
    "        plt.xlabel('Forecast Error')\n",
    "        plt.ylabel('Density')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Cumulative squared errors\n",
    "        plt.subplot(2, 2, 4)\n",
    "        for i, model_name in enumerate(self.models.keys()):\n",
    "            errors = self.forecast_errors[model_name]\n",
    "            valid_errors = errors[~np.isnan(errors)]\n",
    "            if len(valid_errors) > 0:\n",
    "                cumulative_se = np.cumsum(valid_errors**2)\n",
    "                plt.plot(cumulative_se, \n",
    "                        color=colors[i % len(colors)], \n",
    "                        linewidth=2, label=model_name, alpha=0.8)\n",
    "        \n",
    "        plt.title('Cumulative Squared Errors')\n",
    "        plt.xlabel('Forecast Period')\n",
    "        plt.ylabel('Cumulative Squared Error')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_plots:\n",
    "            plt.savefig('rolling_forecast_comparison.png', dpi=300, bbox_inches='tight')\n",
    "            print(\"Plot saved as 'rolling_forecast_comparison.png'\")\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def print_performance_summary(self):\n",
    "        \"\"\"Print a summary of forecast performance metrics\"\"\"\n",
    "        metrics = self.calculate_performance_metrics()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"FORECAST PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Create comparison table\n",
    "        df_metrics = pd.DataFrame(metrics).T\n",
    "        if df_metrics is not None and not df_metrics.empty:\n",
    "            print(df_metrics.round(4))\n",
    "        \n",
    "        print(\"\\nPerformance Ranking by RMSE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        rmse_ranking = []\n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            if model_metrics is not None:\n",
    "                rmse_ranking.append((model_name, model_metrics['RMSE']))\n",
    "        \n",
    "        rmse_ranking.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for i, (model_name, rmse) in enumerate(rmse_ranking, 1):\n",
    "            print(f\"{i}. {model_name}: RMSE = {rmse:.4f}\")\n",
    "        \n",
    "        print(\"\\nPerformance Ranking by MAE:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        mae_ranking = []\n",
    "        for model_name, model_metrics in metrics.items():\n",
    "            if model_metrics is not None:\n",
    "                mae_ranking.append((model_name, model_metrics['MAE']))\n",
    "        \n",
    "        mae_ranking.sort(key=lambda x: x[1])\n",
    "        \n",
    "        for i, (model_name, mae) in enumerate(mae_ranking, 1):\n",
    "            print(f\"{i}. {model_name}: MAE = {mae:.4f}\")\n",
    "\n",
    "\n",
    "def main_rolling_forecast():\n",
    "    \"\"\"\n",
    "    Main function to run the rolling window forecasting comparison\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"ROLLING WINDOW FORECASTING COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load and preprocess data (using your existing function)\n",
    "    y, quartile_prices, _ = main_with_quartiles()\n",
    "    \n",
    "    if y is None:\n",
    "        print(\"Failed to load data. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Define models to compare\n",
    "    # Note: You'll need to import your model classes\n",
    "    models = {\n",
    "        'RobustQLE': RobustQLELocationModel,\n",
    "        'GAS_Location': GAS_Location_Model,\n",
    "        'GAS_t_Location': GAS_t_Location_Model\n",
    "    }\n",
    "    \n",
    "    # Initialize forecaster\n",
    "    window_size = 500  # Adjust based on your data size and requirements\n",
    "    forecaster = RollingWindowForecaster(models, window_size=window_size)\n",
    "    \n",
    "    # Run rolling forecast\n",
    "    start_idx = window_size + 100  # Leave some buffer for initial estimation\n",
    "    results = forecaster.run_rolling_forecast(y, start_idx=start_idx)\n",
    "    \n",
    "    # Print performance summary\n",
    "    forecaster.print_performance_summary()\n",
    "    \n",
    "    # Plot results\n",
    "    forecaster.plot_forecasting_results(plot_window=300, save_plots=True)\n",
    "    \n",
    "    return forecaster, results\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    forecaster, results = main_rolling_forecast()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
