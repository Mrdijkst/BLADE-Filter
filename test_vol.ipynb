{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bac9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize, differential_evolution, basinhopping\n",
    "from typing import Callable, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustQLEVolatilityModel:\n",
    "    \n",
    "    def __init__(self, alpha_loss: float = None, c: float = None):\n",
    "        self.alpha_loss = alpha_loss\n",
    "        self.c = c\n",
    "        self.params = None\n",
    "        self.param_names = ['omega', 'gamma', 'beta']\n",
    "        if alpha_loss is None:\n",
    "            self.param_names.append('alpha_loss')\n",
    "        if c is None:\n",
    "            self.param_names.append('c')\n",
    "        self.fitted_volatility = None\n",
    "        self.residuals = None\n",
    "    \n",
    "    # [Keep all your existing methods: _rho_derivative, _rho_second_derivative, etc.]\n",
    "    # I'm only showing the modified fit method for brevity\n",
    "    import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, Tuple, Optional, Union\n",
    "import autograd \n",
    "from autograd import grad, jacobian\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class RobustQLEVolatilityModel:\n",
    "    \n",
    "    \n",
    "    def __init__(self, alpha_loss: float = None, c: float = None):\n",
    "        \n",
    "        self.alpha_loss = alpha_loss\n",
    "        self.c = c\n",
    "        self.params = None\n",
    "        self.param_names = ['omega', 'gamma', 'beta']\n",
    "        if alpha_loss is None:\n",
    "            self.param_names.append('alpha_loss')\n",
    "        if c is None:\n",
    "            self.param_names.append('c')\n",
    "        self.fitted_volatility = None\n",
    "        self.residuals = None\n",
    "    \n",
    "    def _rho_derivative(self, e: np.ndarray, alpha: float, c: float) -> np.ndarray:\n",
    "        \n",
    "        # Handle scalar inputs by converting to numpy arrays\n",
    "        if np.isscalar(e):\n",
    "            e = np.array([e])\n",
    "            scalar_input = True\n",
    "        else:\n",
    "            scalar_input = False\n",
    "            \n",
    "        if alpha == 2:\n",
    "            # L2 loss (least squares)\n",
    "            result = e / (c**2)\n",
    "        elif alpha == 0:\n",
    "            # Cauchy/Lorentzian loss\n",
    "            result = (2 * e) / (e**2 + 2 * c**2)\n",
    "        elif alpha == float('-inf'):\n",
    "            # Welsch/Leclerc loss\n",
    "            result = (e / c**2) * np.exp(-0.5 * (e/c)**2)\n",
    "        else:\n",
    "            # General case\n",
    "            result = (e / c**2) * np.power((e**2 / c**2) / np.abs(alpha-2) + 1, alpha/2 - 1)\n",
    "        \n",
    "        # Return scalar if input was scalar\n",
    "        if scalar_input:\n",
    "            return result[0]\n",
    "        return result\n",
    "    \n",
    "    #def _rho_second_derivative(self, e: np.ndarray, alpha: float, c: float) -> np.ndarray:\n",
    "        \n",
    "        # Handle scalar inputs by converting to numpy arrays\n",
    "        if np.isscalar(e):\n",
    "            e = np.array([e])\n",
    "            scalar_input = True\n",
    "        else:\n",
    "            scalar_input = False\n",
    "            \n",
    "        if alpha == 2:\n",
    "            # L2 loss (least squares)\n",
    "            result = np.ones_like(e) / (c**2)\n",
    "        elif alpha == 0:\n",
    "            # Cauchy/Lorentzian loss\n",
    "            denom = (1 + 0.5 * (e/c)**2)**2\n",
    "            result = (1 / c**2) * (1 - 0.5 * (e/c)**2) / denom\n",
    "        elif alpha == float('-inf'):\n",
    "            # Welsch/Leclerc loss\n",
    "            result = (1 / c**2) * (1 - (e**2 / c**2)) * np.exp(-0.5 * (e/c)**2)\n",
    "        else:\n",
    "            # General case\n",
    "            base_term = np.power((e/c)**2 / np.abs(alpha-2) + 1, alpha/2 - 2)\n",
    "            part1 = (1 / c**2) * base_term\n",
    "            part2 = (e/c)**2 / np.abs(alpha-2) + 1\n",
    "            part3 = part2 + (alpha/2 - 1) * (2 * e**2) / (c**2 * np.abs(alpha-2))\n",
    "            result = part1 * part3\n",
    "            \n",
    "        \n",
    "        # Return scalar if input was scalar\n",
    "        if scalar_input:\n",
    "            return result[0]\n",
    "        return result\n",
    "    def _rho_second_derivative(self, e: np.ndarray, alpha: float, c: float) -> np.ndarray:\n",
    "        \n",
    "        # Handle scalar inputs by converting to numpy arrays\n",
    "        if np.isscalar(e):\n",
    "            e = np.array([e])\n",
    "            scalar_input = True\n",
    "        else:\n",
    "            scalar_input = False\n",
    "            \n",
    "        if alpha == 2:\n",
    "            # L2 loss (least squares)\n",
    "            result = -np.ones_like(e) / (c**2)\n",
    "        elif alpha == 0:\n",
    "            # Cauchy/Lorentzian loss\n",
    "            denom = (e**2 + 2 * c**2)**2\n",
    "            result = -(2*(e**2 + 2* c**2) - 4 * e **2) / denom\n",
    "        elif alpha == float('-inf'):\n",
    "            # Welsch/Leclerc loss\n",
    "            first_term = ((e)**2 * np.exp( -((e**2)/(2* (c**2)))))/(c**4)\n",
    "            second_term = np.exp( -((e**2)/(2* (c**2))))/(c**2)\n",
    "            result = (first_term - second_term)\n",
    "        else:\n",
    "            # General case\n",
    "            e2 = (e)**2\n",
    "            denom = c**2 * np.abs(alpha - 2)\n",
    "            A = e2 / denom + 1\n",
    "\n",
    "            term1 = -2 * (alpha/2 - 1) * A**(alpha/2 - 2) * e2\n",
    "            term1 /= (c**4 * np.abs(alpha - 2))\n",
    "\n",
    "            term2 = - A**(alpha/2 - 1) / (c**2)\n",
    "\n",
    "            return term1 + term2\n",
    "            \n",
    "        \n",
    "        # Return scalar if input was scalar\n",
    "        if scalar_input:\n",
    "            return result[0]\n",
    "        return result\n",
    "    \n",
    "    #def dpsi_dalpha(self, e_t, c, alpha_loss):\n",
    "        \n",
    "        # define a small neighborhood around the problematic points:\n",
    "        eps = 1e-4\n",
    "\n",
    "        # if α is within ±eps of 2, treat it as exactly 2 (L2 case)\n",
    "        if abs(alpha_loss - 2) < eps:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  # ∂ψ/∂α = 0 for the L2 loss\n",
    "\n",
    "        # similarly guard α≈0\n",
    "        if abs(alpha_loss - 0) < eps:\n",
    "            # use the closed-form ∂ψ/∂α at α=0 (which you've already set to 0)\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c)\n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha\n",
    "\n",
    "        # if you ever parameterize \"α = −∞\" as, say, alpha_loss < some negative cutoff\n",
    "        if alpha_loss < -1e3:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  \n",
    "\n",
    "        # otherwise you're safely away from the singularities, so use your general formula:\n",
    "        abs_alpha_minus_2 = np.abs(alpha_loss - 2)\n",
    "        denominator_inner = c**2 * abs_alpha_minus_2\n",
    "        inner_term = (e_t**2) / denominator_inner + 1\n",
    "        power_term = inner_term**(alpha_loss / 2 - 1)\n",
    "\n",
    "        log_term = np.log(inner_term) / 2\n",
    "        frac_term = (e_t**2 * (alpha_loss / 2 - 1)) / (\n",
    "            c**2 * inner_term * abs_alpha_minus_2 * (alpha_loss - 2)\n",
    "        )\n",
    "\n",
    "        numerator = e_t * power_term * (log_term - frac_term)\n",
    "\n",
    "        return (numerator / c**2)\n",
    "    def dpsi_dalpha(self, e_t, c, alpha_loss):\n",
    "        \n",
    "        # define a small neighborhood around the problematic points:\n",
    "        eps = 1e-4\n",
    "\n",
    "        # if α is within ±eps of 2, treat it as exactly 2 (L2 case)\n",
    "        if abs(alpha_loss - 2) < eps:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  # ∂ψ/∂α = 0 for the L2 loss\n",
    "\n",
    "        # similarly guard α≈0\n",
    "        if abs(alpha_loss - 0) < eps:\n",
    "            # use the closed-form ∂ψ/∂α at α=0 (which you've already set to 0)\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c) \n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha\n",
    "\n",
    "        # if you ever parameterize \"α = −∞\" as, say, alpha_loss < some negative cutoff\n",
    "        if alpha_loss < -1e3:\n",
    "            delta = 1e-5\n",
    "            psi_plus = self._rho_derivative(e_t, alpha_loss + delta, self.c) \n",
    "            psi_minus = self._rho_derivative(e_t, alpha_loss - delta, self.c)\n",
    "            dpsi_dalpha = (psi_plus - psi_minus) / (2 * delta)\n",
    "            return dpsi_dalpha  \n",
    "\n",
    "        # otherwise you're safely away from the singularities, so use your general formula:\n",
    "        alpha= alpha_loss\n",
    "        e2 = e_t**2\n",
    "        denom = c**2 * np.abs(alpha - 2)\n",
    "        A = (e2 / denom) + 1\n",
    "\n",
    "        # Compute the bracket term\n",
    "        term_log = np.log(A) / 2\n",
    "        term_frac = (e2 * (alpha/2 - 1)) / (c**2 * A * np.abs(alpha - 2) * (alpha - 2))\n",
    "        bracket = term_log - term_frac\n",
    "\n",
    "        # Combine everything\n",
    "        result = (e_t * A**(alpha/2 - 1) * bracket) / (c**2)\n",
    "        return result\n",
    "\n",
    "    \n",
    "    #def dpsi_dc(self, e_t, c, alpha_loss):\n",
    "       \n",
    "        eps = 1e-4\n",
    "\n",
    "        if alpha_loss == 2:\n",
    "            # dψ/dc = -2e / c³\n",
    "            return -2 * e_t / c**3\n",
    "\n",
    "        elif abs(alpha_loss) < eps:\n",
    "            # dψ/dc = -8ec / (e² + 2c²)²\n",
    "            return -8 * e_t * c / (e_t**2 + 2 * c**2)**2\n",
    "\n",
    "        elif alpha_loss < -1e3:\n",
    "            # dψ/dc = ( -2e / c³ + e³ / c⁵ ) * exp(-½ (e/c)²)\n",
    "            z = e_t / c\n",
    "            exp_term = np.exp(-0.5 * z**2)\n",
    "            return (-2 * e_t / c**3 + e_t**3 / c**5) * exp_term\n",
    "\n",
    "        else:\n",
    "            # General case:\n",
    "            # dψ/dc = [ -2e / c³ \n",
    "            #           + (e / c²)(α/2 - 1)( (e² / (c²|α−2|) + 1 )**(α/2 - 2) ) * (-2e² / (c³|α−2|)) ]\n",
    "            #         * (e² / (c²|α−2|) + 1 )**(α/2 - 1)\n",
    "            abs_alpha_diff = np.abs(alpha_loss - 2)\n",
    "            z_sq = (e_t**2) / (c**2 * abs_alpha_diff)\n",
    "            base = z_sq + 1\n",
    "            power1 = (alpha_loss / 2) - 2\n",
    "            power2 = (alpha_loss / 2) - 1\n",
    "\n",
    "            first_term = -2 * e_t / c**3\n",
    "            second_term = (e_t / c**2) * ((alpha_loss / 2) - 1) * (base**power1) * (-2 * e_t**2 / (c**3 * abs_alpha_diff))\n",
    "\n",
    "            return (first_term + second_term) * base**power2\n",
    "    def dpsi_dc(self, e_t, c, alpha_loss):\n",
    "       \n",
    "        eps = 1e-4\n",
    "\n",
    "        if alpha_loss == 2:\n",
    "            # dψ/dc = -2e / c³\n",
    "            return -2 * e_t / c**3\n",
    "\n",
    "        elif abs(alpha_loss) < eps:\n",
    "            # dψ/dc = -8ec / (e² + 2c²)²\n",
    "            return -8 * e_t * c / (e_t**2 + 2 * c**2)**2\n",
    "\n",
    "        elif alpha_loss < -1e3:\n",
    "            # dψ/dc = ( -2e / c³ + e³ / c⁵ ) * exp(-½ (e/c)²)\n",
    "            z = e_t / c\n",
    "            exp_term = np.exp(-0.5 * z**2)\n",
    "            return (-2 * e_t / c**3 + e_t**3 / c**5) * exp_term\n",
    "\n",
    "        else:\n",
    "            alpha = alpha_loss\n",
    "            e2 = e_t**2\n",
    "            denom = np.abs(alpha - 2) * c**2\n",
    "            A = e2 / denom + 1\n",
    "\n",
    "            term1 = -2 * e_t * A**(alpha/2 - 1) / (c**3)\n",
    "            term2 = -2 * e_t**3 * (alpha/2 - 1) * A**(alpha/2 - 2) / (np.abs(alpha - 2) * c**5)\n",
    "            \n",
    "            return term1 + term2\n",
    "            # General case:\n",
    "            \n",
    "    \n",
    "    def _filter_volatility(self, y: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "        \n",
    "        T = len(y)\n",
    "        omega, gamma, beta = params[:3]\n",
    "        \n",
    "        param_idx = 3\n",
    "        \n",
    "        # Get alpha_loss parameter\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = params[param_idx]\n",
    "            param_idx += 1\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "        \n",
    "        # Get c parameter\n",
    "        if self.c is None:\n",
    "            c = params[param_idx]\n",
    "        else:\n",
    "            c = self.c\n",
    "        \n",
    "        # Initialize volatility with the unconditional variance\n",
    "        f = np.zeros(T+1)\n",
    "        f[0] = omega/(1-beta)\n",
    "        \n",
    "        # Recursively update the volatility\n",
    "        for t in range(T):\n",
    "            e_t = y[t]**2 - f[t]\n",
    "            psi_t = self._rho_derivative(e_t, alpha_loss, c)   \n",
    "            f[t+1] = omega + gamma * psi_t + beta * f[t]\n",
    "            \n",
    "            # Enforce positive volatility\n",
    "            f[t+1] = max(f[t+1], 1e-12)\n",
    "        \n",
    "        return f[1:]\n",
    "    \n",
    "    def _compute_derivatives(self, y: np.ndarray, f: np.ndarray, params: np.ndarray) -> np.ndarray:\n",
    "       \n",
    "        T = len(y)\n",
    "        omega, gamma, beta = params[:3]\n",
    "        \n",
    "        param_idx = 3\n",
    "        n_params = 3\n",
    "        \n",
    "        # Get alpha_loss parameter\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = params[param_idx]\n",
    "            param_idx += 1\n",
    "            n_params += 1\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "        \n",
    "        # Get c parameter\n",
    "        if self.c is None:\n",
    "            c = params[param_idx]\n",
    "            n_params += 1\n",
    "        else:\n",
    "            c = self.c\n",
    "        \n",
    "        # Initialize derivatives of f_t with respect to theta\n",
    "        e0 = y[0]**2 - f[0]\n",
    "        psi0 =  self._rho_derivative(e0, alpha_loss, c)\n",
    "        d2psi0_df = self._rho_second_derivative(e0, alpha_loss, c) \n",
    "        df_dtheta = np.zeros((T, n_params))\n",
    "        df_dtheta[0, 0] = 1                  # ∂f₁/∂ω\n",
    "        df_dtheta[0, 1] = psi0               # ∂f₁/∂γ\n",
    "        df_dtheta[0, 2] = f[0]               # ∂f₁/∂β\n",
    "        \n",
    "        # Initialize derivatives for alpha_loss and c if they are estimated\n",
    "        param_idx = 3\n",
    "        \n",
    "        if self.alpha_loss is None:\n",
    "            dpsi0_dalpha = self.dpsi_dalpha(e0, c, alpha_loss)\n",
    "            df_dtheta[0, param_idx] = gamma * dpsi0_dalpha  # ∂f₁/∂α\n",
    "            param_idx += 1\n",
    "        \n",
    "        if self.c is None:\n",
    "            dpsi0_dc = self.dpsi_dc(e0, c, alpha_loss)\n",
    "            df_dtheta[0, param_idx] = gamma * dpsi0_dc  # ∂f₁/∂c\n",
    "    \n",
    "        # According to the recursive formula:\n",
    "        \n",
    "        for t in range(1, T):\n",
    "            e_t = (y[t-1]**2 - f[t-1])\n",
    "            psi_t = self._rho_derivative(e_t, alpha_loss, c)  \n",
    "            d2psi_df = self._rho_second_derivative(e_t, alpha_loss, c)   \n",
    "            \n",
    "            # Common term in recursive updates\n",
    "            common_term = gamma * d2psi_df + beta\n",
    "            \n",
    "            # Derivative with respect to parameters\n",
    "            # For omega (∂ω/∂θ_0 = 1, else 0)\n",
    "            df_dtheta[t, 0] = 1 + df_dtheta[t-1, 0] * common_term\n",
    "            \n",
    "            # For gamma (∂γ/∂θ_1 = 1, else 0)\n",
    "            df_dtheta[t, 1] = psi_t + df_dtheta[t-1, 1] * common_term\n",
    "            \n",
    "            # For beta (∂β/∂θ_2 = 1, else 0)\n",
    "            df_dtheta[t, 2] = f[t-1] + df_dtheta[t-1, 2] * common_term\n",
    "            \n",
    "            # Reset param_idx for additional parameters\n",
    "            param_idx = 3\n",
    "            \n",
    "            # Derivative with respect to alpha_loss, if applicable\n",
    "            if self.alpha_loss is None:\n",
    "                dpsi_dalpha = self.dpsi_dalpha(e_t, c, alpha_loss)\n",
    "                df_dtheta[t, param_idx] = gamma * dpsi_dalpha + common_term * df_dtheta[t-1, param_idx]\n",
    "                param_idx += 1\n",
    "            \n",
    "            # Derivative with respect to c, if applicable\n",
    "            if self.c is None:\n",
    "                dpsi_dc = self.dpsi_dc(e_t, c, alpha_loss)\n",
    "                df_dtheta[t, param_idx] = gamma * dpsi_dc + common_term * df_dtheta[t-1, param_idx]\n",
    "        \n",
    "        return df_dtheta\n",
    "    \n",
    "    def _qle_objective(self, params: np.ndarray, y: np.ndarray) -> float:\n",
    "        \n",
    "        try:\n",
    "            # Apply parameter constraints\n",
    "            param_idx = 3\n",
    "            \n",
    "            if self.alpha_loss is None:\n",
    "                # Ensure alpha_loss is in reasonable range\n",
    "                if params[param_idx] < -10 or params[param_idx] > 10:\n",
    "                    return 1e10\n",
    "                param_idx += 1\n",
    "            \n",
    "            if self.c is None:\n",
    "                # Ensure c is positive and reasonable\n",
    "                if params[param_idx] <= 0 or params[param_idx] > 10:\n",
    "                    return 1e10\n",
    "            \n",
    "            # Basic parameter constraints for volatility model stability\n",
    "            if params[0] <= 0 or params[1] < 0 or params[2] < 0 or params[2] >= 1 or params[1] + params[2] >= 0.999:\n",
    "                return 1e10\n",
    "            \n",
    "            # Filter volatility\n",
    "            f = self._filter_volatility(y, params)\n",
    "            \n",
    "            # Compute residuals - h_t is defined as y_t^k - f_t(θ)\n",
    "            h_t = y**2 - f\n",
    "            \n",
    "            # Use f as the conditional variance approximation\n",
    "            sigma2_t = f\n",
    "            \n",
    "            # Compute derivatives of f_t with respect to parameters\n",
    "            df_dtheta = self._compute_derivatives(y, f, params)\n",
    "            \n",
    "            # G_t(θ) = (1/T) * sum[ h_t(θ) / σ²_t(θ) * ∂f_t(θ)/∂θ ]\n",
    "            G_t = np.sum(h_t.reshape(-1, 1) / sigma2_t.reshape(-1, 1) * df_dtheta, axis=0) / len(y)\n",
    "            \n",
    "            # The objective is to minimize ||G_t(θ)||²\n",
    "            obj = np.linalg.norm(G_t)\n",
    "            \n",
    "            return obj\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective function: {e}\")\n",
    "            return 1e10\n",
    "    \n",
    "    def fit(self, y: np.ndarray, initial_params: Optional[Dict] = None, \n",
    "            method: str = 'Nelder-Mead', maxiter: int = 2000) -> Dict:\n",
    "        \n",
    "        # Set default initial parameters if not provided\n",
    "        if initial_params is None:\n",
    "            initial_params = {\n",
    "                'omega': 0.07, \n",
    "                'gamma': 0.11, \n",
    "                'beta': 0.8\n",
    "            }\n",
    "            if self.alpha_loss is None:\n",
    "                initial_params['alpha_loss'] = 1  # Default value close to Cauchy loss\n",
    "            \n",
    "            if self.c is None:\n",
    "                initial_params['c'] = 1.2  # Default value for scale parameter\n",
    "        \n",
    "        # Prepare initial parameter array\n",
    "        init_params = np.array([initial_params[name] for name in self.param_names])\n",
    "        \n",
    "        # Run optimization\n",
    "        options = {'maxiter': maxiter, 'disp': True}\n",
    "        \n",
    "        # Different optimization methods may work better in different cases\n",
    "        if method == 'Nelder-Mead':\n",
    "            options['adaptive'] = True  # Use adaptive Nelder-Mead for better convergence\n",
    "            result = minimize(\n",
    "                self._qle_objective, \n",
    "                init_params, \n",
    "                args=(y,), \n",
    "                method=method, \n",
    "                options=options,\n",
    "            )\n",
    "        elif method == 'BFGS':\n",
    "            options = {'maxiter': maxiter, 'gtol': 1e-6}\n",
    "            result = minimize(\n",
    "                self._qle_objective, \n",
    "                init_params, \n",
    "                args=(y,), \n",
    "                method=method, \n",
    "                options=options,\n",
    "            )\n",
    "        elif method == 'differential_evolution':\n",
    "            # Set up bounds for differential evolution\n",
    "            bounds = []\n",
    "            \n",
    "            # Basic parameters bounds\n",
    "            bounds.extend([(0.001, 0.5), (0.001, 0.5), (0.6, 0.999)])\n",
    "            \n",
    "            # Alpha bounds if estimated\n",
    "            if self.alpha_loss is None:\n",
    "                bounds.append((-5, 5))\n",
    "            \n",
    "            # c bounds if estimated\n",
    "            if self.c is None:\n",
    "                bounds.append((0.1, 5.0))\n",
    "            \n",
    "            result = differential_evolution(\n",
    "                self._qle_objective,\n",
    "                bounds=bounds,\n",
    "                args=(y,),\n",
    "                strategy='best1bin',\n",
    "                maxiter=maxiter,\n",
    "                disp=True,\n",
    "                polish=True\n",
    "            )\n",
    "        \n",
    "        if not result.success and method != 'differential_evolution':\n",
    "            print(f\"Warning: Optimization did not converge: {result.message}\")\n",
    "            \n",
    "            # Try again with different method if first one failed\n",
    "            if method == 'Nelder-Mead':\n",
    "                print(\"Trying BFGS method instead...\")\n",
    "                result = minimize(\n",
    "                    self._qle_objective,\n",
    "                    init_params,\n",
    "                    args=(y,),\n",
    "                    method='BFGS',\n",
    "                    options={'maxiter': maxiter}\n",
    "                )\n",
    "        \n",
    "        # Store parameters\n",
    "        self.params = {name: val for name, val in zip(self.param_names, result.x)}\n",
    "        \n",
    "        # Compute fitted volatility\n",
    "        param_array = np.array([self.params[name] for name in self.param_names])\n",
    "        self.fitted_volatility = self._filter_volatility(y, param_array)\n",
    "        self.residuals = y**2 - self.fitted_volatility\n",
    "        \n",
    "        print(f\"Optimization result: {result.message}\")\n",
    "        print(f\"Parameters: {self.params}\")\n",
    "        \n",
    "        return self.params    \n",
    "    def _parameter_bounds_check(self, params: np.ndarray) -> bool:\n",
    "        \"\"\"Check if parameters are within valid bounds\"\"\"\n",
    "        param_idx = 3\n",
    "        \n",
    "        # Basic parameter constraints\n",
    "        if params[0] <= 0 or params[1] < 0 or params[2] < 0 or params[2] >= 1:\n",
    "            return False\n",
    "        if params[1] + params[2] >= 0.999:\n",
    "            return False\n",
    "            \n",
    "        if self.alpha_loss is None:\n",
    "            if params[param_idx] < -10 or params[param_idx] > 10:\n",
    "                return False\n",
    "            param_idx += 1\n",
    "        \n",
    "        if self.c is None:\n",
    "            if params[param_idx] <= 0 or params[param_idx] > 10:\n",
    "                return False\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    def _generate_random_start(self, bounds: list = None) -> np.ndarray:\n",
    "        \"\"\"Generate random starting parameters within bounds\"\"\"\n",
    "        if bounds is None:\n",
    "            # Default bounds\n",
    "            omega_bound = (0.001, 0.2)\n",
    "            gamma_bound = (0.01, 0.5)\n",
    "            beta_bound = (0.1, 0.95)\n",
    "            \n",
    "            params = [\n",
    "                np.random.uniform(*omega_bound),\n",
    "                np.random.uniform(*gamma_bound),\n",
    "                np.random.uniform(*beta_bound)\n",
    "            ]\n",
    "            \n",
    "            if self.alpha_loss is None:\n",
    "                params.append(np.random.uniform(-3, 3))\n",
    "            \n",
    "            if self.c is None:\n",
    "                params.append(np.random.uniform(0.5, 3.0))\n",
    "                \n",
    "        else:\n",
    "            params = [np.random.uniform(low, high) for low, high in bounds]\n",
    "        \n",
    "        return np.array(params)\n",
    "    \n",
    "    def _multi_start_optimization(self, y: np.ndarray, n_starts: int = 10, \n",
    "                                method: str = 'L-BFGS-B', maxiter: int = 1000) -> Dict:\n",
    "        \"\"\"Run optimization from multiple random starting points\"\"\"\n",
    "        \n",
    "        # Set up bounds for constrained optimization\n",
    "        bounds = []\n",
    "        bounds.extend([(0.001, 0.5), (0.001, 0.5), (0.1, 0.999)])\n",
    "        \n",
    "        if self.alpha_loss is None:\n",
    "            bounds.append((-5, 5))\n",
    "        \n",
    "        if self.c is None:\n",
    "            bounds.append((0.1, 5.0))\n",
    "        \n",
    "        best_result = None\n",
    "        best_obj = float('inf')\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"Running {n_starts} optimizations from different starting points...\")\n",
    "        \n",
    "        for i in range(n_starts):\n",
    "            try:\n",
    "                # Generate random starting point\n",
    "                init_params = self._generate_random_start(bounds)\n",
    "                \n",
    "                # Run optimization\n",
    "                result = minimize(\n",
    "                    self._qle_objective,\n",
    "                    init_params,\n",
    "                    args=(y,),\n",
    "                    method=method,\n",
    "                    bounds=bounds,\n",
    "                    options={'maxiter': maxiter, 'disp': False}\n",
    "                )\n",
    "                \n",
    "                all_results.append(result)\n",
    "                \n",
    "                if result.fun < best_obj:\n",
    "                    best_obj = result.fun\n",
    "                    best_result = result\n",
    "                    \n",
    "                print(f\"  Start {i+1}: obj={result.fun:.6f}, success={result.success}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Start {i+1}: Failed with error {e}\")\n",
    "                continue\n",
    "        \n",
    "        return best_result, all_results\n",
    "    \n",
    "    def fit(self, y: np.ndarray, initial_params: Optional[Dict] = None, \n",
    "            method: str = 'basin_hopping', maxiter: int = 2000, \n",
    "            n_starts: int = 5, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Fit the model with various optimization strategies\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            'basin_hopping', 'multi_start', 'differential_evolution', 'Nelder-Mead', 'L-BFGS-B'\n",
    "        n_starts : int\n",
    "            Number of starting points for multi_start method\n",
    "        **kwargs : additional arguments for specific methods\n",
    "        \"\"\"\n",
    "        \n",
    "        # Set default initial parameters\n",
    "        if initial_params is None:\n",
    "            initial_params = {\n",
    "                'omega': 0.07, \n",
    "                'gamma': 0.11, \n",
    "                'beta': 0.8\n",
    "            }\n",
    "            if self.alpha_loss is None:\n",
    "                initial_params['alpha_loss'] = 1.0\n",
    "            if self.c is None:\n",
    "                initial_params['c'] = 1.2\n",
    "        \n",
    "        init_params = np.array([initial_params[name] for name in self.param_names])\n",
    "        \n",
    "        # Set up bounds for constrained methods\n",
    "        bounds = []\n",
    "        bounds.extend([(0.001, 0.5), (0.001, 0.5), (0.1, 0.999)])\n",
    "        \n",
    "        if self.alpha_loss is None:\n",
    "            bounds.append((-5, 5))\n",
    "        \n",
    "        if self.c is None:\n",
    "            bounds.append((0.1, 5.0))\n",
    "        \n",
    "        print(f\"Fitting model using {method} method...\")\n",
    "        \n",
    "        if method == 'basin_hopping':\n",
    "            # Basin-hopping with custom step-taking and acceptance criteria\n",
    "            \n",
    "            class BoundedStep:\n",
    "                def __init__(self, bounds, stepsize=0.1):\n",
    "                    self.bounds = np.array(bounds)\n",
    "                    self.stepsize = stepsize\n",
    "                \n",
    "                def __call__(self, x):\n",
    "                    \"\"\"Take a random step within bounds\"\"\"\n",
    "                    # Generate random perturbation\n",
    "                    perturbation = np.random.uniform(-self.stepsize, self.stepsize, len(x))\n",
    "                    \n",
    "                    # Apply perturbation\n",
    "                    x_new = x + perturbation\n",
    "                    \n",
    "                    # Clip to bounds\n",
    "                    x_new = np.clip(x_new, self.bounds[:, 0], self.bounds[:, 1])\n",
    "                    \n",
    "                    return x_new\n",
    "            \n",
    "            def accept_test(f_new, x_new, f_old, x_old):\n",
    "                \"\"\"Custom acceptance criterion\"\"\"\n",
    "                # Always accept if better\n",
    "                if f_new < f_old:\n",
    "                    return True\n",
    "                # Accept with probability based on improvement\n",
    "                delta = f_new - f_old\n",
    "                prob = np.exp(-delta * 10)  # Adjust temperature\n",
    "                return np.random.random() < prob\n",
    "            \n",
    "            # Create step-taking class\n",
    "            step_class = BoundedStep(bounds, stepsize=kwargs.get('stepsize', 0.1))\n",
    "            \n",
    "            # Run basin-hopping\n",
    "            result = basinhopping(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                minimizer_kwargs={\n",
    "                    'method': 'L-BFGS-B',\n",
    "                    'bounds': bounds,\n",
    "                    'args': (y,),\n",
    "                    'options': {'maxiter': 200}\n",
    "                },\n",
    "                niter=kwargs.get('niter', 100),\n",
    "                T=kwargs.get('T', 1.0),\n",
    "                stepsize=kwargs.get('stepsize', 0.1),\n",
    "                take_step=step_class,\n",
    "                accept_test=accept_test,\n",
    "                disp=True\n",
    "            )\n",
    "            \n",
    "        elif method == 'multi_start':\n",
    "            result, all_results = self._multi_start_optimization(\n",
    "                y, n_starts=n_starts, maxiter=maxiter\n",
    "            )\n",
    "            \n",
    "        elif method == 'differential_evolution':\n",
    "            result = differential_evolution(\n",
    "                self._qle_objective,\n",
    "                bounds=bounds,\n",
    "                args=(y,),\n",
    "                strategy='best1bin',\n",
    "                maxiter=maxiter,\n",
    "                disp=True,\n",
    "                polish=True,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "        elif method in ['L-BFGS-B', 'SLSQP']:\n",
    "            result = minimize(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                args=(y,),\n",
    "                method=method,\n",
    "                bounds=bounds,\n",
    "                options={'maxiter': maxiter, 'disp': True}\n",
    "            )\n",
    "            \n",
    "        else:  # Nelder-Mead or other unconstrained methods\n",
    "            options = {'maxiter': maxiter, 'disp': True}\n",
    "            if method == 'Nelder-Mead':\n",
    "                options['adaptive'] = True\n",
    "                \n",
    "            result = minimize(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                args=(y,),\n",
    "                method=method,\n",
    "                options=options\n",
    "            )\n",
    "        \n",
    "        # Handle failed optimization\n",
    "        if not result.success and method not in ['differential_evolution', 'basin_hopping']:\n",
    "            print(f\"Warning: {method} optimization failed: {result.message}\")\n",
    "            print(\"Trying basin-hopping as fallback...\")\n",
    "            \n",
    "            # Fallback to basin-hopping\n",
    "            step_class = BoundedStep(bounds, stepsize=0.05)\n",
    "            result = basinhopping(\n",
    "                self._qle_objective,\n",
    "                init_params,\n",
    "                minimizer_kwargs={\n",
    "                    'method': 'L-BFGS-B',\n",
    "                    'bounds': bounds,\n",
    "                    'args': (y,),\n",
    "                    'options': {'maxiter': 200}\n",
    "                },\n",
    "                niter=50,\n",
    "                take_step=step_class,\n",
    "                disp=True\n",
    "            )\n",
    "        \n",
    "        # Store results\n",
    "        self.params = {name: val for name, val in zip(self.param_names, result.x)}\n",
    "        \n",
    "        # Compute fitted volatility\n",
    "        param_array = np.array([self.params[name] for name in self.param_names])\n",
    "        self.fitted_volatility = self._filter_volatility(y, param_array)\n",
    "        self.residuals = y**2 - self.fitted_volatility\n",
    "        \n",
    "        print(f\"Optimization result: {result.message if hasattr(result, 'message') else 'Completed'}\")\n",
    "        print(f\"Final objective value: {result.fun:.8f}\")\n",
    "        print(f\"Parameters: {self.params}\")\n",
    "        \n",
    "        return self.params\n",
    "    \n",
    "    def compare_methods(self, y: np.ndarray, methods: list = None, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Compare different optimization methods\n",
    "        \n",
    "        Returns dictionary with results from each method\n",
    "        \"\"\"\n",
    "        if methods is None:\n",
    "            methods = ['L-BFGS-B', 'basin_hopping', 'differential_evolution', 'multi_start']\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for method in methods:\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"Testing method: {method}\")\n",
    "            print(f\"{'='*50}\")\n",
    "            \n",
    "            try:\n",
    "                # Reset parameters\n",
    "                self.params = None\n",
    "                self.fitted_volatility = None\n",
    "                self.residuals = None\n",
    "                \n",
    "                # Fit model\n",
    "                params = self.fit(y, method=method, **kwargs)\n",
    "                \n",
    "                # Store results\n",
    "                results[method] = {\n",
    "                    'params': params.copy(),\n",
    "                    'objective': self._qle_objective(\n",
    "                        np.array([params[name] for name in self.param_names]), y\n",
    "                    ),\n",
    "                    'success': True\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Method {method} failed: {e}\")\n",
    "                results[method] = {\n",
    "                    'params': None,\n",
    "                    'objective': float('inf'),\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "    \n",
    "        return results\n",
    "    def plot_volatility(self, y: np.ndarray, true_vol: np.ndarray = None, title: str = \"Estimated Volatility\") -> None:\n",
    "        \n",
    "        if self.fitted_volatility is None:\n",
    "            raise ValueError(\"Model must be fit before plotting\")\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Plot original data\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(y, 'b-', alpha=0.5, label='Returns')\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot volatility\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(np.sqrt(self.fitted_volatility), 'r-', label='Estimated Volatility')\n",
    "        \n",
    "        if true_vol is not None:\n",
    "            plt.plot(np.sqrt(true_vol), 'b-', alpha=0.3, label='True Volatility')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.ylim(0, np.max(np.sqrt(self.fitted_volatility)) * 1.5)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    def simulate(self, T: int, dist: str = 't', df: int = 5, seed: int = None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \n",
    "        if self.params is None:\n",
    "            raise ValueError(\"Parameters must be set before simulation\")\n",
    "\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        omega = self.params['omega']\n",
    "        gamma = self.params['gamma']\n",
    "        beta = self.params['beta']\n",
    "\n",
    "        if self.alpha_loss is None:\n",
    "            alpha_loss = self.params['alpha_loss']\n",
    "        else:\n",
    "            alpha_loss = self.alpha_loss\n",
    "\n",
    "        if self.c is None:\n",
    "            c = self.params['c']\n",
    "        else:\n",
    "            c = self.c\n",
    "\n",
    "        # Initialize arrays\n",
    "        y = np.zeros(T)\n",
    "        f = np.zeros(T+1)\n",
    "        f[0] = omega / (1 - beta)  # Start at unconditional variance\n",
    "\n",
    "        # Generate innovations\n",
    "        if dist == 't':\n",
    "            eps = np.random.standard_t(df, T)\n",
    "        else:\n",
    "            eps = np.random.normal(0, 1, T)\n",
    "            \n",
    "        # Generate data\n",
    "        for t in range(T):\n",
    "            # Generate return\n",
    "            y[t] = np.sqrt(f[t]) * eps[t]\n",
    "            \n",
    "            # Compute the score\n",
    "            e_t = (y[t]**2 - f[t])\n",
    "            psi_t = self._rho_derivative(e_t, alpha_loss, c) \n",
    "            \n",
    "            # Update volatility\n",
    "            f[t+1] = omega + gamma * psi_t + beta * f[t]\n",
    "            #f[t+1] = max(f[t+1], 1e-12)  # Ensure positive volatility\n",
    "            \n",
    "        return y, f[1:]\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264d631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting standard QLE model (alpha=2):\n",
      "Fitting model using basin_hopping method...\n",
      "basinhopping step 0: f 2.03787e-05\n",
      "basinhopping step 1: f 2.19051e-05 trial_f 2.19051e-05 accepted 1  lowest_f 2.03787e-05\n",
      "basinhopping step 2: f 2.19051e-05 trial_f 1e+10 accepted 0  lowest_f 2.03787e-05\n",
      "basinhopping step 3: f 2.45878e-05 trial_f 2.45878e-05 accepted 1  lowest_f 2.03787e-05\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 4: f 2.45878e-05 trial_f 8.85855e-05 accepted 0  lowest_f 2.03787e-05\n",
      "basinhopping step 5: f 1.01992e-05 trial_f 1.01992e-05 accepted 1  lowest_f 1.01992e-05\n",
      "found new global minimum on step 5 with function value 1.01992e-05\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 6: f 1.01992e-05 trial_f 8.31591e-05 accepted 0  lowest_f 1.01992e-05\n",
      "warning: basinhopping: local minimization failure\n",
      "basinhopping step 7: f 1.01992e-05 trial_f nan accepted 0  lowest_f 1.01992e-05\n",
      "basinhopping step 8: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 9: f 0.00510591 trial_f 0.00510591 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 10: f 0.00463718 trial_f 0.00463718 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 11: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 12: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 13: f 0.0056354 trial_f 0.0056354 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 14: f 0.00478497 trial_f 0.00478497 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 15: f 0.00470561 trial_f 0.00470561 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 16: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 17: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 18: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 19: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 20: f 0.00461855 trial_f 0.00461855 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 21: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 22: f 0.00619452 trial_f 0.00619452 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 23: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 24: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 25: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 26: f 0.00558179 trial_f 0.00558179 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 27: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 28: f 0.00462707 trial_f 0.00462707 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 29: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 30: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 31: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 32: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 33: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 34: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 35: f 0.00503456 trial_f 0.00503456 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 36: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 37: f 0.0046533 trial_f 0.0046533 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 38: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 39: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 40: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 41: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 42: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 43: f 0.0046123 trial_f 0.0046123 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 44: f 0.00463006 trial_f 0.00463006 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 45: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 46: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n",
      "basinhopping step 47: f 0.00460309 trial_f 0.00460309 accepted 1  lowest_f 1.01992e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict, Tuple, Optional, Union\n",
    "import autograd \n",
    "from autograd import grad, jacobian\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "def demo_with_outliers():\n",
    "    \"\"\"Demonstrate the model's performance with outliers in the data\"\"\"\n",
    "    # Set parameters for the data generating process\n",
    "    true_params = {\n",
    "        'omega': 0.07,\n",
    "        'gamma': 0.11,\n",
    "        'beta': 0.80,\n",
    "        'alpha_loss': 1,\n",
    "        'c': 1.2\n",
    "    }\n",
    "    \n",
    "    # Create a model with the true parameters for simulation\n",
    "    sim_model = RobustQLEVolatilityModel(alpha_loss=true_params['alpha_loss'])\n",
    "    sim_model.params = true_params\n",
    "    \n",
    "    # Simulate clean data\n",
    "    T = 6000\n",
    "    y_clean, true_vol = sim_model.simulate(T, dist='n', df=7, seed=42)\n",
    "    \n",
    "    # Create a copy of the data with outliers\n",
    "    y_outliers = y_clean.copy()\n",
    "    \n",
    "    # Add outliers at different positions and with different magnitudes\n",
    "    \n",
    "    # 1. Add a few isolated very large outliers\n",
    "    np.random.seed(123)\n",
    "    outlier_positions = np.random.choice(range(500, T-500), 10, replace=False)\n",
    "    outlier_signs = np.random.choice([-1, 1], 10)\n",
    "    \n",
    "    for i, pos in enumerate(outlier_positions):\n",
    "        # Create outliers that are 10-15 standard deviations from the mean\n",
    "        local_std = np.sqrt(true_vol[pos])\n",
    "        y_outliers[pos] = outlier_signs[i] * np.random.uniform(10, 15) * local_std\n",
    "    \n",
    "    # 2. Add a cluster of moderately large outliers (simulating a volatility burst)\n",
    "    cluster_start = 3000\n",
    "    for i in range(30):\n",
    "        local_std = np.sqrt(true_vol[cluster_start + i])\n",
    "        if i % 2 == 0:  # every other observation is an outlier\n",
    "            y_outliers[cluster_start + i] = np.random.choice([-1, 1]) * np.random.uniform(5, 8) * local_std\n",
    "    \n",
    "    # Create models for comparison\n",
    "    # L2 loss model (standard QLE, not robust)\n",
    "    model_l2 = RobustQLEVolatilityModel(alpha_loss=2)\n",
    "    \n",
    "    # Cauchy loss model (should be robust)\n",
    "    model_cauchy = RobustQLEVolatilityModel(alpha_loss=0)\n",
    "    \n",
    "    # Alpha=1 model (should be robust)\n",
    "    model_alpha1 = RobustQLEVolatilityModel(alpha_loss=1)\n",
    "    \n",
    "    # Model with estimated alpha\n",
    "    model_est = RobustQLEVolatilityModel()\n",
    "    \n",
    "    # Fit the models to the data with outliers\n",
    "    print(\"\\nFitting standard QLE model (alpha=2):\")\n",
    "    params_l2 = model_l2.fit(y_outliers, method='basin_hopping', niter=100, stepsize=0.1)\n",
    "    \n",
    "    print(\"\\nFitting robust model with Cauchy loss (alpha=0):\")\n",
    "    params_cauchy = model_cauchy.fit(y_outliers,method='basin_hopping', niter=100, stepsize=0.1)\n",
    "    \n",
    "    print(\"\\nFitting robust model with alpha=1:\")\n",
    "    params_alpha1 = model_alpha1.fit(y_outliers, method='basin_hopping', niter=100, stepsize=0.1)\n",
    "    \n",
    "    print(\"\\nFitting model with estimated alpha:\")\n",
    "    params_est = model_est.fit(y_outliers, method='basin_hopping', niter=100, stepsize=0.1)\n",
    "    \n",
    "    # Compare volatility estimates\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(14, 16), sharex=True)\n",
    "    \n",
    "    # Plot returns with outliers\n",
    "    axes[0].plot(y_outliers, 'b-', alpha=0.5)\n",
    "    axes[0].set_title('Returns with Outliers')\n",
    "    axes[0].set_ylabel('Returns')\n",
    "    \n",
    "    # Highlight outlier positions\n",
    "    for pos in outlier_positions:\n",
    "        axes[0].axvline(x=pos, color='r', linestyle='--', alpha=0.3)\n",
    "    axes[0].axvspan(cluster_start, cluster_start + 30, color='r', alpha=0.1)\n",
    "    \n",
    "    # Plot estimated volatilities\n",
    "    axes[1].plot(np.sqrt(model_l2.fitted_volatility), 'r-', label='L2 Loss (α=2)')\n",
    "    axes[1].plot(np.sqrt(true_vol), 'k--', alpha=0.5, label='True Volatility')\n",
    "    axes[1].set_title('L2 Loss (Standard QLE)')\n",
    "    axes[1].set_ylabel('Volatility')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    axes[2].plot(np.sqrt(model_cauchy.fitted_volatility), 'g-', label='Cauchy Loss (α=0)')\n",
    "    axes[2].plot(np.sqrt(true_vol), 'k--', alpha=0.5, label='True Volatility')\n",
    "    axes[2].set_title('Cauchy Loss (Robust QLE)')\n",
    "    axes[2].set_ylabel('Volatility')\n",
    "    axes[2].legend()\n",
    "    \n",
    "    axes[3].plot(np.sqrt(model_alpha1.fitted_volatility), 'b-', label='α=1')\n",
    "    axes[3].plot(np.sqrt(model_est.fitted_volatility), 'm-', label=f'Est. α={params_est[\"alpha_loss\"]:.2f}')\n",
    "    axes[3].plot(np.sqrt(true_vol), 'k--', alpha=0.5, label='True Volatility')\n",
    "    axes[3].set_title('Comparison of Different Alpha Values')\n",
    "    axes[3].set_ylabel('Volatility')\n",
    "    axes[3].set_xlabel('Time')\n",
    "    axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Zoom in on outlier cluster region\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    window_start = cluster_start - 100\n",
    "    window_end = cluster_start + 150\n",
    "    \n",
    "    # Plot returns with outliers (zoomed)\n",
    "    axes[0].plot(range(window_start, window_end), y_outliers[window_start:window_end], 'b-', alpha=0.7)\n",
    "    axes[0].set_title('Returns with Outliers (Zoomed)')\n",
    "    axes[0].set_ylabel('Returns')\n",
    "    \n",
    "    # Plot estimated volatilities (zoomed)\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_l2.fitted_volatility[window_start:window_end]), \n",
    "                 'r-', label='L2 Loss (α=2)')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_cauchy.fitted_volatility[window_start:window_end]), \n",
    "                 'g-', label='Cauchy Loss (α=0)')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_alpha1.fitted_volatility[window_start:window_end]), \n",
    "                 'b-', label='α=1')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_est.fitted_volatility[window_start:window_end]), \n",
    "                 'm-', label=f'Est. α={params_est[\"alpha_loss\"]:.2f}')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(true_vol[window_start:window_end]), \n",
    "                 'k--', alpha=0.5, label='True Volatility')\n",
    "    axes[1].set_title('Volatility Estimates Near Outlier Cluster')\n",
    "    axes[1].set_ylabel('Volatility')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # zoom in on isolated outliers\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "    window_start = outlier_positions[0] - 100\n",
    "    window_end = outlier_positions[0] + 100\n",
    "    # Plot returns with outliers (zoomed)\n",
    "    axes[0].plot(range(window_start, window_end), y_outliers[window_start:window_end], 'b-', alpha=0.7)\n",
    "    axes[0].set_title('Returns with Outliers (Zoomed)')\n",
    "    axes[0].set_ylabel('Returns')\n",
    "    # Highlight outlier positions\n",
    "    for pos in outlier_positions:\n",
    "        axes[0].axvline(x=pos, color='r', linestyle='--', alpha=0.3)\n",
    "    # Plot estimated volatilities (zoomed)\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_l2.fitted_volatility[window_start:window_end]),\n",
    "                 'r-', label='L2 Loss (α=2)')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_cauchy.fitted_volatility[window_start:window_end]),\n",
    "                 'g-', label='Cauchy Loss (α=0)')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_alpha1.fitted_volatility[window_start:window_end]),\n",
    "                 'b-', label='α=1')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(model_est.fitted_volatility[window_start:window_end]),\n",
    "                 'm-', label=f'Est. α={params_est[\"alpha_loss\"]:.2f}')\n",
    "    axes[1].plot(range(window_start, window_end), np.sqrt(true_vol[window_start:window_end]),\n",
    "                 'k--', alpha=0.5, label='True Volatility')\n",
    "    axes[1].set_title('Volatility Estimates Near Isolated Outliers')\n",
    "    axes[1].set_ylabel('Volatility')\n",
    "    axes[1].set_xlabel('Time')\n",
    "    axes[1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()  \n",
    "    \n",
    "    # Calculate RMSE for each model\n",
    "    rmse_l2 = np.sqrt(np.mean((model_l2.fitted_volatility - true_vol)**2))\n",
    "    rmse_cauchy = np.sqrt(np.mean((model_cauchy.fitted_volatility - true_vol)**2))\n",
    "    rmse_alpha1 = np.sqrt(np.mean((model_alpha1.fitted_volatility - true_vol)**2))\n",
    "    rmse_est = np.sqrt(np.mean((model_est.fitted_volatility - true_vol)**2))\n",
    "    \n",
    "    print(\"\\nRMSE Comparison:\")\n",
    "    print(f\"L2 Loss (α=2): {rmse_l2:.6f}\")\n",
    "    print(f\"Cauchy Loss (α=0): {rmse_cauchy:.6f}\")\n",
    "    print(f\"Alpha=1: {rmse_alpha1:.6f}\")\n",
    "    print(f\"Estimated Alpha (α={params_est['alpha_loss']:.2f}): {rmse_est:.6f}\")\n",
    "    \n",
    "    # Calculate mean absolute error near outliers\n",
    "    near_outliers = np.zeros(T, dtype=bool)\n",
    "    for pos in outlier_positions:\n",
    "        # Mark 5 observations before and after each outlier\n",
    "        start = max(0, pos-5)\n",
    "        end = min(T, pos+6)\n",
    "        near_outliers[start:end] = True\n",
    "    \n",
    "    # Also mark the cluster region\n",
    "    near_outliers[cluster_start:cluster_start+40] = True\n",
    "    \n",
    "    # Calculate MAE near outliers\n",
    "    mae_l2_near = np.mean(np.abs(model_l2.fitted_volatility[near_outliers] - true_vol[near_outliers]))\n",
    "    mae_cauchy_near = np.mean(np.abs(model_cauchy.fitted_volatility[near_outliers] - true_vol[near_outliers]))\n",
    "    mae_alpha1_near = np.mean(np.abs(model_alpha1.fitted_volatility[near_outliers] - true_vol[near_outliers]))\n",
    "    mae_est_near = np.mean(np.abs(model_est.fitted_volatility[near_outliers] - true_vol[near_outliers]))\n",
    "    \n",
    "    print(\"\\nMAE Near Outliers:\")\n",
    "    print(f\"L2 Loss (α=2): {mae_l2_near:.6f}\")\n",
    "    print(f\"Cauchy Loss (α=0): {mae_cauchy_near:.6f}\")\n",
    "    print(f\"Alpha=1: {mae_alpha1_near:.6f}\")\n",
    "    print(f\"Estimated Alpha (α={params_est['alpha_loss']:.2f}): {mae_est_near:.6f}\")\n",
    "    \n",
    "    return model_l2, model_cauchy, model_alpha1, model_est, y_outliers, true_vol\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_l2, model_cauchy, model_alpha1, model_est, y_outliers, true_vol = demo_with_outliers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
